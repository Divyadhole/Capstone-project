{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxEnt Distribution Modeling\n",
    "\n",
    "This notebook implements species distribution modeling using MaxEnt and projects distributions to past time periods.\n",
    "\n",
    "## Steps:\n",
    "1. Prepare occurrence data for MaxEnt\n",
    "2. Prepare climate layers for MaxEnt\n",
    "3. Run MaxEnt via command line\n",
    "4. Project to past time periods\n",
    "5. Generate distribution maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:35.395029Z",
     "iopub.status.busy": "2025-12-03T08:39:35.394611Z",
     "iopub.status.idle": "2025-12-03T08:39:35.433449Z",
     "shell.execute_reply": "2025-12-03T08:39:35.432890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully\n",
      "Project root: /Users/divyadhole/Capstone-project\n",
      "Data directory: /Users/divyadhole/Capstone-project/data\n",
      "Output directory: /Users/divyadhole/Capstone-project/outputs\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration with absolute path\n",
    "CONFIG_PATH = Path(\"/Users/divyadhole/Capstone-project/config.yaml\")\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"/Users/divyadhole/Capstone-project\")\n",
    "DATA_DIR = PROJECT_ROOT / config['data_dir']\n",
    "OUTPUT_DIR = PROJECT_ROOT / config['output_dir']\n",
    "MAXENT_DIR = PROJECT_ROOT / config['maxent_dir']\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DATA_DIR, OUTPUT_DIR, MAXENT_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Configuration loaded successfully\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:35.437920Z",
     "iopub.status.busy": "2025-12-03T08:39:35.437558Z",
     "iopub.status.idle": "2025-12-03T08:39:35.444480Z",
     "shell.execute_reply": "2025-12-03T08:39:35.443978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: True\n",
      "bio1*.tif → 0 matches\n",
      "bio_1*.tif → 11 matches\n",
      "  sample: bio_18.tif\n",
      "bio12*.tif → 0 matches\n",
      "bio_12*.tif → 1 matches\n",
      "  sample: bio_12.tif\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "manual_climate_dir = Path(\"/Users/divyadhole/Capstone-project/data/paleoclim/LH_v1_2_5m\")\n",
    "print(\"Directory exists:\", manual_climate_dir.exists())\n",
    "\n",
    "for pattern in (\"bio1*.tif\", \"bio_1*.tif\", \"bio12*.tif\", \"bio_12*.tif\"):\n",
    "    matches = list(manual_climate_dir.glob(pattern))\n",
    "    print(pattern, \"→\", len(matches), \"matches\")\n",
    "    if matches:\n",
    "        print(\"  sample:\", matches[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:35.447480Z",
     "iopub.status.busy": "2025-12-03T08:39:35.447234Z",
     "iopub.status.idle": "2025-12-03T08:39:35.781453Z",
     "shell.execute_reply": "2025-12-03T08:39:35.780520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 19 climate layers for MaxEnt.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "manual_climate_dir = Path(\"/Users/divyadhole/Capstone-project/data/paleoclim/LH_v1_2_5m\")\n",
    "MAXENT_LAYERS = OUTPUT_DIR / \"maxent_layers\"\n",
    "MAXENT_LAYERS.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "layer_files = []\n",
    "for tif in manual_climate_dir.glob(\"bio_*.tif\"):\n",
    "    dest = MAXENT_LAYERS / f\"{manual_climate_dir.name}_{tif.name}\"\n",
    "    shutil.copy(tif, dest)\n",
    "    layer_files.append(dest)\n",
    "\n",
    "print(f\"Copied {len(layer_files)} climate layers for MaxEnt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare occurrence data for MaxEnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:35.786628Z",
     "iopub.status.busy": "2025-12-03T08:39:35.786244Z",
     "iopub.status.idle": "2025-12-03T08:39:35.797895Z",
     "shell.execute_reply": "2025-12-03T08:39:35.797323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current climate directories: ['/Users/divyadhole/Capstone-project/data/paleoclim/LH_v1_2_5m']\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Load configuration\n",
    "CONFIG_PATH = Path(\"/Users/divyadhole/Capstone-project/config.yaml\")\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"/Users/divyadhole/Capstone-project\")\n",
    "DATA_DIR = PROJECT_ROOT / config['data_dir']\n",
    "OUTPUT_DIR = PROJECT_ROOT / config['output_dir']\n",
    "MAXENT_DIR = PROJECT_ROOT / config['maxent_dir']\n",
    "\n",
    "# MaxEnt specific configuration\n",
    "MAXENT_CONFIG = {\n",
    "    \"current_climate_dirs\": [\n",
    "        PROJECT_ROOT / \"data/paleoclim/LH_v1_2_5m\"  # Last Glacial Maximum\n",
    "    ],\n",
    "    \"projection_climate_dirs\": [\n",
    "        PROJECT_ROOT / \"data/paleoclim/EH_v1_2_5m\",  # Early Holocene\n",
    "        PROJECT_ROOT / \"data/paleoclim/LGM_v1_2_5m\"  # Last Glacial Maximum\n",
    "    ],\n",
    "    \"variables\": [\"bio1\", \"bio12\"],  # Temperature and Precipitation\n",
    "    \"maxent_jar\": PROJECT_ROOT / \"maxent/maxent.jar\",\n",
    "    \"output_dir\": OUTPUT_DIR / \"maxent_outputs\"\n",
    "}\n",
    "\n",
    "def gather_current_climate_dirs():\n",
    "    \"\"\"Gather current climate directories from config.\"\"\"\n",
    "    configured = MAXENT_CONFIG.get(\"current_climate_dirs\", [])\n",
    "    resolved = []\n",
    "    for path in configured:\n",
    "        candidate_path = Path(path)\n",
    "        if not candidate_path.is_absolute():\n",
    "            candidate_path = PROJECT_ROOT / candidate_path\n",
    "        if candidate_path.exists():\n",
    "            resolved.append(str(candidate_path))\n",
    "    return resolved\n",
    "\n",
    "# Create required directories\n",
    "for directory in [DATA_DIR, OUTPUT_DIR, MAXENT_DIR, MAXENT_CONFIG[\"output_dir\"]]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Now you can safely call\n",
    "current_climate_paths = gather_current_climate_dirs()\n",
    "print(\"Current climate directories:\", current_climate_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:35.801814Z",
     "iopub.status.busy": "2025-12-03T08:39:35.801441Z",
     "iopub.status.idle": "2025-12-03T08:39:35.845913Z",
     "shell.execute_reply": "2025-12-03T08:39:35.845459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 2 climate layers for MaxEnt → /Users/divyadhole/Capstone-project/outputs/maxent_layers\n",
      "Successfully prepared 2 layers in /Users/divyadhole/Capstone-project/outputs/maxent_layers\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Load main configuration\n",
    "CONFIG_PATH = Path(\"/Users/divyadhole/Capstone-project/config.yaml\")\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set up base paths\n",
    "PROJECT_ROOT = Path(\"/Users/divyadhole/Capstone-project\")\n",
    "OUTPUT_DIR = PROJECT_ROOT / config['output_dir']\n",
    "PALEO_BASE_DIR = PROJECT_ROOT / \"data/paleoclim\"\n",
    "\n",
    "# MaxEnt Configuration\n",
    "MAXENT_CONFIG = {\n",
    "    \"current_climate_dirs\": [\n",
    "        PALEO_BASE_DIR / \"LH_v1_2_5m\"  # Current climate\n",
    "    ],\n",
    "    \"projection_climate_dirs\": [\n",
    "        PALEO_BASE_DIR / \"EH_v1_2_5m\",  # Early Holocene\n",
    "        PALEO_BASE_DIR / \"LGM_v1_2_5m\"  # Last Glacial Maximum\n",
    "    ],\n",
    "    \"variables\": [\"bio1\", \"bio12\"],  # Temperature and Precipitation\n",
    "    \"output_dir\": OUTPUT_DIR / \"maxent_outputs\"\n",
    "}\n",
    "\n",
    "# CLIMATE_CONFIG for backward compatibility\n",
    "CLIMATE_CONFIG = {\n",
    "    \"current\": PALEO_BASE_DIR / \"LH_v1_2_5m\",\n",
    "    \"paleo\": PALEO_BASE_DIR,\n",
    "    \"variables\": MAXENT_CONFIG[\"variables\"]\n",
    "}\n",
    "\n",
    "def _variant_names(var):\n",
    "    \"\"\"Generate possible filename variants for a climate variable.\"\"\"\n",
    "    names = {var}\n",
    "    if var.startswith(\"bio\"):\n",
    "        suffix = var[3:]\n",
    "        if suffix.startswith(\"_\"):\n",
    "            suffix = suffix[1:]\n",
    "        names.update({f\"bio{suffix}\", f\"bio_{suffix}\"})\n",
    "    return list(names)\n",
    "\n",
    "def prepare_maxent_layers(climate_dirs, output_dir, variables, allow_all_bio=False):\n",
    "    \"\"\"Copy climate layers (GeoTIFF/ASCII) for MaxEnt.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    layer_files = []\n",
    "    for climate_dir in climate_dirs:\n",
    "        climate_dir = Path(climate_dir)\n",
    "        if not climate_dir.exists():\n",
    "            print(f\"Skipping missing climate directory: {climate_dir}\")\n",
    "            continue\n",
    "\n",
    "        for var in variables:\n",
    "            candidate_names = [\"bio_\"] if allow_all_bio else _variant_names(var)\n",
    "            var_files = []\n",
    "            for name in candidate_names:\n",
    "                if name == \"bio_\":\n",
    "                    var_files.extend(climate_dir.glob(\"bio_*.tif\"))\n",
    "                    var_files.extend(climate_dir.glob(\"bio_*.asc\"))\n",
    "                else:\n",
    "                    var_files.extend(climate_dir.glob(f\"{name}.tif\"))\n",
    "                    var_files.extend(climate_dir.glob(f\"{name}.asc\"))\n",
    "\n",
    "            if not var_files:\n",
    "                print(f\"  Missing layers for {var} in {climate_dir}\")\n",
    "                continue\n",
    "\n",
    "            for var_file in var_files:\n",
    "                dest_file = output_dir / f\"{climate_dir.name}_{var_file.name}\"\n",
    "                shutil.copy(var_file, dest_file)\n",
    "                layer_files.append(dest_file)\n",
    "\n",
    "    if not layer_files:\n",
    "        raise FileNotFoundError(\"No climate layers were prepared. Check climate directories and variable names.\")\n",
    "\n",
    "    print(f\"Prepared {len(layer_files)} climate layers for MaxEnt → {output_dir}\")\n",
    "    return layer_files\n",
    "\n",
    "def gather_current_climate_dirs():\n",
    "    \"\"\"Gather current climate directories with fallbacks.\"\"\"\n",
    "    configured = MAXENT_CONFIG.get(\"current_climate_dirs\", [])\n",
    "    if isinstance(configured, (str, Path)):\n",
    "        configured = [configured]\n",
    "\n",
    "    fallback_candidates = []\n",
    "    if CLIMATE_CONFIG.get(\"current\"):\n",
    "        fallback_candidates.append(CLIMATE_CONFIG[\"current\"])\n",
    "    \n",
    "    # Add common subdirectories to check\n",
    "    fallback_candidates.extend(PALEO_BASE_DIR / name for name in (\"current\", \"present\", \"LH_v1_2_5m\"))\n",
    "\n",
    "    resolved = []\n",
    "    for candidate in configured + fallback_candidates:\n",
    "        if not candidate:\n",
    "            continue\n",
    "        candidate_path = Path(candidate).expanduser()\n",
    "        if candidate_path.exists() and candidate_path not in resolved:\n",
    "            resolved.append(candidate_path)\n",
    "    return resolved\n",
    "\n",
    "# Create required directories\n",
    "MAXENT_LAYERS = OUTPUT_DIR / \"maxent_layers\"\n",
    "MAXENT_LAYERS.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Get climate data paths\n",
    "current_climate_paths = gather_current_climate_dirs()\n",
    "variables = MAXENT_CONFIG.get('variables', ['bio1', 'bio12'])\n",
    "\n",
    "# Process climate layers\n",
    "if current_climate_paths:\n",
    "    try:\n",
    "        layer_files = prepare_maxent_layers(current_climate_paths, MAXENT_LAYERS, variables)\n",
    "        print(f\"Successfully prepared {len(layer_files)} layers in {MAXENT_LAYERS}\")\n",
    "    except FileNotFoundError as exc:\n",
    "        print(exc)\n",
    "        print(\"Falling back to copying all bio_* layers...\")\n",
    "        try:\n",
    "            layer_files = prepare_maxent_layers(\n",
    "                current_climate_paths, \n",
    "                MAXENT_LAYERS, \n",
    "                variables, \n",
    "                allow_all_bio=True\n",
    "            )\n",
    "            print(f\"Successfully prepared {len(layer_files)} layers in {MAXENT_LAYERS}\")\n",
    "        except Exception as exc2:\n",
    "            print(\"Failed to prepare layers:\", exc2)\n",
    "else:\n",
    "    print(f\"No climate directories found. Please check your configuration in {CONFIG_PATH}\")\n",
    "    print(\"Expected climate data in:\", PALEO_BASE_DIR / \"LH_v1_2_5m/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Project to past time periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:35.849134Z",
     "iopub.status.busy": "2025-12-03T08:39:35.848803Z",
     "iopub.status.idle": "2025-12-03T08:39:36.066757Z",
     "shell.execute_reply": "2025-12-03T08:39:36.064761Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m species_name \u001b[38;5;241m=\u001b[39m MAXENT_CONFIG\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecies_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     maxent_presence \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_maxent_occurrences\u001b[49m\u001b[43m(\u001b[49m\u001b[43moccurrence_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxent_occurrences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     50\u001b[0m     maxent_presence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mprepare_maxent_occurrences\u001b[0;34m(occurrence_file, output_file, species_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m occurrence_file\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOccurrence file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moccurrence_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(occurrence_file)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOccurrence table is empty; rerun Notebook 02 or provide valid points.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1 – Prepare occurrence data for MaxEnt\n",
    "def prepare_maxent_occurrences(occurrence_file, output_file, species_name=None):\n",
    "    \"\"\"Format occurrence data for MaxEnt (CSV: species, longitude, latitude).\"\"\"\n",
    "    occurrence_file = Path(occurrence_file)\n",
    "    output_file = Path(output_file)\n",
    "\n",
    "    if not occurrence_file.exists():\n",
    "        raise FileNotFoundError(f\"Occurrence file not found: {occurrence_file}\")\n",
    "\n",
    "    df = pd.read_csv(occurrence_file)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Occurrence table is empty; rerun Notebook 02 or provide valid points.\")\n",
    "\n",
    "    lon_cols = [c for c in df.columns if c.lower() in {\"lon\", \"longitude\", \"decimallongitude\"}]\n",
    "    lat_cols = [c for c in df.columns if c.lower() in {\"lat\", \"latitude\", \"decimallatitude\"}]\n",
    "    if not lon_cols or not lat_cols:\n",
    "        raise ValueError(\"Occurrence data must contain longitude/latitude columns.\")\n",
    "\n",
    "    lon_col = lon_cols[0]\n",
    "    lat_col = lat_cols[0]\n",
    "\n",
    "    if \"species\" not in df.columns:\n",
    "        if \"scientific_name\" in df.columns:\n",
    "            df[\"species\"] = df[\"scientific_name\"].fillna(species_name or \"target_species\")\n",
    "        elif {\"GENUS\", \"SPECIES\"}.issubset(df.columns):\n",
    "            df[\"species\"] = df[\"GENUS\"].str.strip() + \" \" + df[\"SPECIES\"].str.strip()\n",
    "        else:\n",
    "            df[\"species\"] = species_name or \"target_species\"\n",
    "\n",
    "    maxent_df = (\n",
    "        df[[\"species\", lon_col, lat_col]]\n",
    "        .rename(columns={lon_col: \"longitude\", lat_col: \"latitude\"})\n",
    "        .dropna(subset=[\"longitude\", \"latitude\"])\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    if maxent_df.empty:\n",
    "        raise ValueError(\"No valid occurrence rows after filtering missing coordinates.\")\n",
    "\n",
    "    maxent_df.to_csv(output_file, index=False, header=False)\n",
    "    print(f\"Prepared {len(maxent_df)} occurrence points for MaxEnt → {output_file}\")\n",
    "    return maxent_df\n",
    "\n",
    "occurrence_file = Path(MAXENT_CONFIG.get(\"occurrence_file\", OUTPUT_DIR / \"occurrence_points_gbif.csv\")).expanduser()\n",
    "maxent_occurrences = OUTPUT_DIR / \"maxent_occurrences.csv\"\n",
    "species_name = MAXENT_CONFIG.get(\"species_name\")\n",
    "\n",
    "try:\n",
    "    maxent_presence = prepare_maxent_occurrences(occurrence_file, maxent_occurrences, species_name)\n",
    "except (FileNotFoundError, ValueError) as exc:\n",
    "    maxent_presence = None\n",
    "    print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:36.071546Z",
     "iopub.status.busy": "2025-12-03T08:39:36.071074Z",
     "iopub.status.idle": "2025-12-03T08:39:36.123501Z",
     "shell.execute_reply": "2025-12-03T08:39:36.116107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 2 climate layers for MaxEnt → /Users/divyadhole/Capstone-project/outputs/maxent_layers\n"
     ]
    }
   ],
   "source": [
    "# Step 2 – Prepare climate layers for MaxEnt\n",
    "def _variant_names(var):\n",
    "    names = {var}\n",
    "    if var.startswith(\"bio\"):\n",
    "        suffix = var[3:]\n",
    "        if suffix.startswith(\"_\"):\n",
    "            suffix = suffix[1:]\n",
    "        names.update({f\"bio{suffix}\", f\"bio_{suffix}\"})\n",
    "    return list(names)\n",
    "\n",
    "def prepare_maxent_layers(climate_dirs, output_dir, variables, allow_all_bio=False):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    layer_files = []\n",
    "\n",
    "    for climate_dir in map(Path, climate_dirs):\n",
    "        if not climate_dir.exists():\n",
    "            print(f\"Skipping missing climate directory: {climate_dir}\")\n",
    "            continue\n",
    "\n",
    "        for var in variables:\n",
    "            candidate_names = [\"bio_\"] if allow_all_bio else _variant_names(var)\n",
    "            var_files = []\n",
    "            for name in candidate_names:\n",
    "                if name == \"bio_\":\n",
    "                    var_files.extend(climate_dir.glob(\"bio_*.tif\"))\n",
    "                    var_files.extend(climate_dir.glob(\"bio_*.asc\"))\n",
    "                else:\n",
    "                    var_files.extend(climate_dir.glob(f\"{name}.tif\"))\n",
    "                    var_files.extend(climate_dir.glob(f\"{name}.asc\"))\n",
    "\n",
    "            if not var_files:\n",
    "                print(f\"  Missing layers for {var} in {climate_dir}\")\n",
    "                continue\n",
    "\n",
    "            for var_file in var_files:\n",
    "                dest_file = output_dir / f\"{climate_dir.name}_{var_file.name}\"\n",
    "                shutil.copy(var_file, dest_file)\n",
    "                layer_files.append(dest_file)\n",
    "\n",
    "    if not layer_files:\n",
    "        raise FileNotFoundError(\"No climate layers were prepared. Check climate directories and variable names.\")\n",
    "\n",
    "    print(f\"Prepared {len(layer_files)} climate layers for MaxEnt → {output_dir}\")\n",
    "    return layer_files\n",
    "\n",
    "def gather_current_climate_dirs():\n",
    "    configured = MAXENT_CONFIG.get(\"current_climate_dirs\")\n",
    "    if isinstance(configured, (str, Path)):\n",
    "        configured = [configured]\n",
    "    configured = configured or []\n",
    "\n",
    "    fallback = []\n",
    "    if CLIMATE_CONFIG.get(\"current\"):\n",
    "        fallback.append(CLIMATE_CONFIG[\"current\"])\n",
    "    fallback.extend(PALEO_BASE_DIR / name for name in (\"current\", \"present\", \"LH_v1_2_5m\"))\n",
    "\n",
    "    resolved = []\n",
    "    for candidate in configured + fallback:\n",
    "        if not candidate:\n",
    "            continue\n",
    "        candidate_path = Path(candidate).expanduser()\n",
    "        if candidate_path.exists() and candidate_path not in resolved:\n",
    "            resolved.append(candidate_path)\n",
    "    return resolved\n",
    "\n",
    "current_climate_paths = gather_current_climate_dirs()\n",
    "MAXENT_LAYERS = OUTPUT_DIR / \"maxent_layers\"\n",
    "variables = CLIMATE_CONFIG.get(\"variables\", [\"bio1\", \"bio12\"])\n",
    "\n",
    "if current_climate_paths:\n",
    "    try:\n",
    "        layer_files = prepare_maxent_layers(current_climate_paths, MAXENT_LAYERS, variables)\n",
    "    except FileNotFoundError as exc:\n",
    "        print(exc)\n",
    "        print(\"Falling back to copying all bio_* layers from current directory…\")\n",
    "        try:\n",
    "            layer_files = prepare_maxent_layers(current_climate_paths, MAXENT_LAYERS, variables, allow_all_bio=True)\n",
    "        except FileNotFoundError as exc2:\n",
    "            layer_files = None\n",
    "            print(exc2)\n",
    "else:\n",
    "    layer_files = None\n",
    "    print(\"No usable current climate directories found. Set maxent_params.current_climate_dirs or add data to\", PALEO_BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fresh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:36.131134Z",
     "iopub.status.busy": "2025-12-03T08:39:36.130287Z",
     "iopub.status.idle": "2025-12-03T08:39:37.885863Z",
     "shell.execute_reply": "2025-12-03T08:39:37.885460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MaxEnt Distribution Modeling\n",
      "Project root: /Users/divyadhole/Capstone-project\n",
      "Data directory: /Users/divyadhole/Capstone-project/data\n",
      "Output directory: /Users/divyadhole/Capstone-project/outputs\n",
      "Paleo climate base directory: /Users/divyadhole/Capstone-project/data/paleoclim\n",
      "No valid occurrence files found for example_species\n",
      "Failed to load occurrence data. Please check the data directory and file format.\n"
     ]
    }
   ],
   "source": [
    "# 1. Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# 2. Set up directories and configuration\n",
    "def setup_directories():\n",
    "    # Use absolute path to config file\n",
    "    CONFIG_PATH = Path(\"/Users/divyadhole/Capstone-project/config.yaml\")\n",
    "    \n",
    "    # Read config file\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Set up directories\n",
    "    PROJECT_ROOT = CONFIG_PATH.parent\n",
    "    DATA_DIR = PROJECT_ROOT / config.get('data_dir', 'data')\n",
    "    OUTPUT_DIR = PROJECT_ROOT / config.get('output_dir', 'outputs')\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Get configuration with defaults\n",
    "    CLIMATE_CONFIG = config.get('climate_data', {})\n",
    "    MAXENT_CONFIG = config.get('maxent_params', {})\n",
    "    \n",
    "    # Set up climate directories\n",
    "    PALEO_BASE_DIR = PROJECT_ROOT / CLIMATE_CONFIG.get('paleo_base', 'data/paleoclim')\n",
    "    PALEO_BASE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Ensure maxent output directory exists\n",
    "    maxent_output = OUTPUT_DIR / \"maxent_outputs\"\n",
    "    maxent_output.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    print(f\"Project root: {PROJECT_ROOT}\")\n",
    "    print(f\"Data directory: {DATA_DIR}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"Paleo climate base directory: {PALEO_BASE_DIR}\")\n",
    "    \n",
    "    return PROJECT_ROOT, DATA_DIR, OUTPUT_DIR, PALEO_BASE_DIR, CLIMATE_CONFIG, MAXENT_CONFIG\n",
    "\n",
    "# 3. Load and prepare occurrence data\n",
    "def load_occurrence_data(data_dir, species_name):\n",
    "    \"\"\"Load and prepare species occurrence data\"\"\"\n",
    "    # Try multiple possible file locations and extensions\n",
    "    possible_files = [\n",
    "        data_dir / f\"{species_name}_occurrences.csv\",\n",
    "        data_dir / \"occurrence_data\" / f\"{species_name}.csv\",\n",
    "        data_dir / \"occurrences\" / f\"{species_name}.csv\"\n",
    "    ]\n",
    "    \n",
    "    df = None\n",
    "    for occurrence_file in possible_files:\n",
    "        if occurrence_file.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(occurrence_file)\n",
    "                print(f\"Loaded {len(df)} occurrence records from {occurrence_file}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {occurrence_file}: {e}\")\n",
    "    \n",
    "    if df is None:\n",
    "        print(f\"No valid occurrence files found for {species_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Basic data validation\n",
    "    required_columns = ['species', 'decimalLongitude', 'decimalLatitude']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "        return None\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 4. Main execution\n",
    "def main():\n",
    "    print(\"Starting MaxEnt Distribution Modeling\")\n",
    "    \n",
    "    try:\n",
    "        # Set up directories\n",
    "        PROJECT_ROOT, DATA_DIR, OUTPUT_DIR, PALEO_BASE_DIR, CLIMATE_CONFIG, MAXENT_CONFIG = setup_directories()\n",
    "        \n",
    "        # Example species name - update this as needed\n",
    "        SPECIES_NAME = \"example_species\"\n",
    "        \n",
    "        # Load occurrence data\n",
    "        occurrences = load_occurrence_data(DATA_DIR, SPECIES_NAME)\n",
    "        if occurrences is None:\n",
    "            print(\"Failed to load occurrence data. Please check the data directory and file format.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nAvailable data preview:\")\n",
    "        print(occurrences.head())\n",
    "        print(f\"\\nSetup complete. Ready to proceed with MaxEnt modeling.\")\n",
    "        print(f\"Output will be saved to: {OUTPUT_DIR}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:37.890851Z",
     "iopub.status.busy": "2025-12-03T08:39:37.889238Z",
     "iopub.status.idle": "2025-12-03T08:39:37.899236Z",
     "shell.execute_reply": "2025-12-03T08:39:37.898672Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_species_occurrences(species_name, iucn_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Extract occurrence data for a specific species from IUCN shapefiles.\n",
    "    \n",
    "    Args:\n",
    "        species_name (str): Scientific name of the species (e.g., 'Rhinopithecus roxellana')\n",
    "        iucn_dir (Path): Directory containing IUCN shapefiles\n",
    "        output_dir (Path): Directory to save the output CSV\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved CSV file with occurrence data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import geopandas as gpd\n",
    "    except ImportError:\n",
    "        print(\"Installing geopandas...\")\n",
    "        subprocess.check_call([\"pip\", \"install\", \"geopandas\"])\n",
    "        import geopandas as gpd\n",
    "    \n",
    "    # Check if shapefiles exist\n",
    "    shapefile_parts = [\n",
    "        iucn_dir / \"MAMMALS_PART1.shp\",\n",
    "        iucn_dir / \"MAMMALS_PART2.shp\"\n",
    "    ]\n",
    "    \n",
    "    if not all(f.exists() for f in shapefile_parts):\n",
    "        raise FileNotFoundError(\"Could not find IUCN shapefiles in the specified directory\")\n",
    "    \n",
    "    # Read and combine both shapefiles\n",
    "    print(\"Reading IUCN shapefiles...\")\n",
    "    dfs = []\n",
    "    for shp in shapefile_parts:\n",
    "        df = gpd.read_file(shp)\n",
    "        print(f\"Columns in {shp.name}: {df.columns.tolist()}\")\n",
    "        dfs.append(df)\n",
    "    \n",
    "    all_species = gpd.GeoDataFrame(pd.concat(dfs, ignore_index=True))\n",
    "    \n",
    "    # Find the column that contains species names\n",
    "    possible_name_columns = ['binomial', 'sci_name', 'scientific', 'name', 'species']\n",
    "    name_column = None\n",
    "    \n",
    "    for col in possible_name_columns:\n",
    "        if col in all_species.columns:\n",
    "            name_column = col\n",
    "            break\n",
    "    \n",
    "    if name_column is None:\n",
    "        print(\"Could not find species name column. Available columns:\")\n",
    "        print(all_species.columns.tolist())\n",
    "        return None\n",
    "    \n",
    "    print(f\"Using column '{name_column}' for species names\")\n",
    "    \n",
    "    # Filter for our target species (case-insensitive)\n",
    "    species_data = all_species[\n",
    "        all_species[name_column].str.lower() == species_name.lower()\n",
    "    ]\n",
    "    \n",
    "    if len(species_data) == 0:\n",
    "        print(f\"\\nNo records found for {species_name}\")\n",
    "        print(\"Available species (first 20):\")\n",
    "        print(all_species[name_column].unique()[:20])\n",
    "        return None\n",
    "    \n",
    "    # Select and rename columns\n",
    "    output_data = species_data[[name_column, 'geometry']].copy()\n",
    "    output_data['decimalLongitude'] = output_data.geometry.centroid.x\n",
    "    output_data['decimalLatitude'] = output_data.geometry.centroid.y\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = output_dir / f\"{species_name.replace(' ', '_')}_occurrences.csv\"\n",
    "    output_data[['binomial', 'decimalLongitude', 'decimalLatitude']].to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nFound {len(output_data)} occurrence records for {species_name}\")\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:37.902028Z",
     "iopub.status.busy": "2025-12-03T08:39:37.901673Z",
     "iopub.status.idle": "2025-12-03T08:39:37.905433Z",
     "shell.execute_reply": "2025-12-03T08:39:37.904958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/divyadhole/Capstone-project/notebooks\n",
      "Contents of data directory: [PosixPath('data/VCF_files')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(\"Current working directory:\", Path.cwd())\n",
    "print(\"Contents of data directory:\", list(Path(\"data\").glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:37.908385Z",
     "iopub.status.busy": "2025-12-03T08:39:37.908035Z",
     "iopub.status.idle": "2025-12-03T08:39:38.101078Z",
     "shell.execute_reply": "2025-12-03T08:39:38.098956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Enhanced Distribution Modeling Analysis\n",
      "Project root: /Users/divyadhole/Capstone-project\n",
      "Data directory: /Users/divyadhole/Capstone-project/data\n",
      "Output directory: /Users/divyadhole/Capstone-project/outputs\n",
      "Paleo climate base directory: /Users/divyadhole/Capstone-project/data/paleoclim\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 195\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 161\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Enhanced Distribution Modeling Analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Set up directories\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m DATA_DIR, OUTPUT_DIR, PALEO_BASE_DIR, CLIMATE_CONFIG, MAXENT_CONFIG \u001b[38;5;241m=\u001b[39m setup_directories()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Extract species occurrences\u001b[39;00m\n\u001b[1;32m    164\u001b[0m SPECIES_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRhinopithecus_roxellana\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "def fix_periods_directory_structure(climate_dir):\n",
    "    \"\"\"Ensure the climate directory has the correct structure.\"\"\"\n",
    "    climate_dir = Path(climate_dir)\n",
    "    if not (climate_dir / \"current\").exists():\n",
    "        # If current directory doesn't exist, the main directory might be the current data\n",
    "        if any(f.name.startswith('bio_') for f in climate_dir.glob('*.tif')):\n",
    "            # This is actually the current data directory\n",
    "            current_dir = climate_dir.parent / \"current\"\n",
    "            current_dir.mkdir(exist_ok=True)\n",
    "            # Move files to current directory\n",
    "            for f in climate_dir.glob('*.tif'):\n",
    "                f.rename(current_dir / f.name)\n",
    "            print(f\"Moved climate data to {current_dir}\")\n",
    "        else:\n",
    "            # Check if there are subdirectories that could be periods\n",
    "            potential_periods = [d for d in climate_dir.iterdir() if d.is_dir()]\n",
    "            if not potential_periods:\n",
    "                raise FileNotFoundError(f\"No climate data found in {climate_dir}\")\n",
    "            print(f\"Found potential period directories: {[p.name for p in potential_periods]}\")\n",
    "    return climate_dir\n",
    "\n",
    "def create_enhanced_visualization(prediction_path, occurrence_file, output_dir):\n",
    "    \"\"\"Create an enhanced visualization of the prediction with better styling.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create a custom colormap\n",
    "    colors = [(0, 0.3, 0), (0, 0.7, 0), (1, 1, 0), (1, 0.7, 0), (1, 0, 0)]  # Green to yellow to red\n",
    "    cmap_name = 'suitability'\n",
    "    cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)\n",
    "    \n",
    "    with rasterio.open(prediction_path) as src:\n",
    "        pred = src.read(1)\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # Plot prediction with better styling\n",
    "        im = ax.imshow(pred, cmap=cm, \n",
    "                      vmin=0, vmax=1,\n",
    "                      interpolation='bilinear')\n",
    "        \n",
    "        # Add colorbar with better formatting\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Habitat Suitability', fontsize=12)\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        \n",
    "        # Read and plot occurrence points\n",
    "        occurrences = pd.read_csv(occurrence_file)\n",
    "        coords = occurrences[['decimalLongitude', 'decimalLatitude']].values\n",
    "        rows, cols = rasterio.transform.rowcol(transform, coords[:, 0], coords[:, 1])\n",
    "        \n",
    "        # Plot occurrence points with better styling\n",
    "        ax.scatter(cols, rows, c='white', s=100, edgecolor='black', \n",
    "                  linewidth=1.5, label='Occurrence Points', zorder=5)\n",
    "        \n",
    "        # Add scale bar\n",
    "        from matplotlib_scalebar.scalebar import ScaleBar\n",
    "        ax.add_artist(ScaleBar(1, location='lower right', \n",
    "                             length_fraction=0.2, \n",
    "                             height_fraction=0.02,\n",
    "                             border_pad=1,\n",
    "                             color='black',\n",
    "                             box_color='white',\n",
    "                             box_alpha=0.7))\n",
    "        \n",
    "        # Add north arrow\n",
    "        x, y, arrow_length = 0.95, 0.95, 0.1\n",
    "        ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),\n",
    "                   arrowprops=dict(facecolor='black', width=5, headwidth=15),\n",
    "                   ha='center', va='center', fontsize=20,\n",
    "                   xycoords=ax.transAxes)\n",
    "        \n",
    "        # Add title and adjust layout\n",
    "        species_name = Path(occurrence_file).stem.replace('_occurrences', '').replace('_', ' ')\n",
    "        ax.set_title(f'Predicted Habitat Suitability\\n{species_name.title()}', \n",
    "                    fontsize=14, pad=20)\n",
    "        ax.legend(loc='upper left', frameon=True, framealpha=0.8)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Save high-quality figure\n",
    "        output_path = Path(output_dir) / 'enhanced_prediction.png'\n",
    "        plt.savefig(output_path, bbox_inches='tight', dpi=300, facecolor='white')\n",
    "        plt.close()\n",
    "        \n",
    "        return output_path\n",
    "\n",
    "def run_enhanced_analysis(occurrence_file, climate_dir, output_dir, species_name):\n",
    "    \"\"\"Run the enhanced analysis with visualization and projections.\"\"\"\n",
    "    # Fix directory structure if needed\n",
    "    climate_dir = fix_periods_directory_structure(climate_dir)\n",
    "    \n",
    "    # Run the model\n",
    "    results_dir = run_maxent_sklearn(\n",
    "        occurrence_file=occurrence_file,\n",
    "        climate_dir=str(climate_dir),\n",
    "        output_dir=output_dir,\n",
    "        species_name=species_name\n",
    "    )\n",
    "    \n",
    "    if not results_dir:\n",
    "        print(\"Model run failed. Cannot proceed with visualization.\")\n",
    "        return None\n",
    "    \n",
    "    results_dir = Path(results_dir)\n",
    "    \n",
    "    # Enhanced visualization\n",
    "    prediction_path = results_dir / \"prediction.tif\"\n",
    "    if prediction_path.exists():\n",
    "        print(\"\\nGenerating enhanced visualization...\")\n",
    "        try:\n",
    "            viz_path = create_enhanced_visualization(\n",
    "                prediction_path=prediction_path,\n",
    "                occurrence_file=occurrence_file,\n",
    "                output_dir=results_dir\n",
    "            )\n",
    "            print(f\"Visualization saved to: {viz_path}\")\n",
    "            \n",
    "            # Display the visualization\n",
    "            from IPython.display import Image, display\n",
    "            display(Image(filename=str(viz_path)))\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating visualization: {str(e)}\")\n",
    "    \n",
    "    # Project to other time periods\n",
    "    print(\"\\nProjecting to other time periods...\")\n",
    "    periods_dir = climate_dir.parent if (climate_dir / \"current\").exists() else climate_dir\n",
    "    \n",
    "    if periods_dir.exists():\n",
    "        period_dirs = [d for d in periods_dir.iterdir() \n",
    "                      if d.is_dir() and d.name != 'current' and d.name != 'maxent_results']\n",
    "        \n",
    "        if period_dirs:\n",
    "            print(f\"Found {len(period_dirs)} time periods to project to\")\n",
    "            \n",
    "            # Create projections directory\n",
    "            projections_dir = results_dir / \"projections\"\n",
    "            projections_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Load the model (you'll need to modify run_maxent_sklearn to return/save the model)\n",
    "            # For now, we'll just create a placeholder\n",
    "            print(\"Projection functionality needs model saving/loading to be implemented\")\n",
    "            print(f\"Would project to: {[d.name for d in period_dirs]}\")\n",
    "        else:\n",
    "            print(f\"No period directories found in {periods_dir}\")\n",
    "    else:\n",
    "        print(f\"Could not find periods directory at {periods_dir}\")\n",
    "    \n",
    "    print(\"\\nAnalysis completed!\")\n",
    "    print(f\"Results available in: {results_dir}\")\n",
    "    return results_dir\n",
    "\n",
    "# Update main function\n",
    "def main():\n",
    "    print(\"Starting Enhanced Distribution Modeling Analysis\")\n",
    "    \n",
    "    # Set up directories\n",
    "    DATA_DIR, OUTPUT_DIR, PALEO_BASE_DIR, CLIMATE_CONFIG, MAXENT_CONFIG = setup_directories()\n",
    "    \n",
    "    # Extract species occurrences\n",
    "    SPECIES_NAME = \"Rhinopithecus_roxellana\"\n",
    "    print(f\"\\nExtracting occurrence data for {SPECIES_NAME}...\")\n",
    "    occurrence_file = extract_species_occurrences(\n",
    "        \"Rhinopithecus roxellana\",\n",
    "        DATA_DIR / \"MAMMALS\",\n",
    "        OUTPUT_DIR / \"occurrences\"\n",
    "    )\n",
    "    \n",
    "    if occurrence_file:\n",
    "        print(\"\\nRunning enhanced analysis...\")\n",
    "        climate_dir = OUTPUT_DIR / \"climate\"\n",
    "        \n",
    "        # Ensure climate directory exists\n",
    "        if not climate_dir.exists():\n",
    "            print(f\"Climate directory not found at {climate_dir}\")\n",
    "            print(\"Please ensure climate data is prepared in the correct structure.\")\n",
    "            return\n",
    "        \n",
    "        results = run_enhanced_analysis(\n",
    "            occurrence_file=occurrence_file,\n",
    "            climate_dir=climate_dir,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            species_name=SPECIES_NAME\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\nAnalysis completed successfully!\")\n",
    "            print(f\"Results saved to: {results}\")\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:38.106639Z",
     "iopub.status.busy": "2025-12-03T08:39:38.106179Z",
     "iopub.status.idle": "2025-12-03T08:39:38.121155Z",
     "shell.execute_reply": "2025-12-03T08:39:38.119276Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_species_occurrences(species_name, iucn_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Extract species occurrence data from IUCN shapefile.\n",
    "    \n",
    "    Args:\n",
    "        species_name (str): Scientific name of the species (e.g., \"Rhinopithecus roxellana\")\n",
    "        iucn_dir (Path): Directory containing IUCN shapefiles\n",
    "        output_dir (Path): Directory to save the output CSV\n",
    "    \n",
    "    Returns:\n",
    "        Path: Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    import geopandas as gpd\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Try different possible shapefile names\n",
    "    shp_files = list(Path(iucn_dir).rglob(\"*.shp\"))\n",
    "    if not shp_files:\n",
    "        raise FileNotFoundError(f\"No shapefiles found in {iucn_dir}\")\n",
    "    \n",
    "    # Try to find the species in each shapefile\n",
    "    species_found = False\n",
    "    output_data = None\n",
    "    \n",
    "    for shp_file in shp_files:\n",
    "        try:\n",
    "            print(f\"Searching in {shp_file}...\")\n",
    "            gdf = gpd.read_file(shp_file)\n",
    "            \n",
    "            # Try different possible column names for species name\n",
    "            name_columns = ['binomial', 'BINOMIAL', 'SCINAME', 'sci_name', 'species', 'name']\n",
    "            name_col = None\n",
    "            \n",
    "            for col in name_columns:\n",
    "                if col in gdf.columns:\n",
    "                    name_col = col\n",
    "                    break\n",
    "            \n",
    "            if name_col is None:\n",
    "                print(f\"  Could not find species name column in {shp_file}\")\n",
    "                print(f\"  Available columns: {gdf.columns.tolist()}\")\n",
    "                continue\n",
    "                \n",
    "            # Filter for our species\n",
    "            species_data = gdf[gdf[name_col].str.lower() == species_name.lower()]\n",
    "            \n",
    "            if not species_data.empty:\n",
    "                print(f\"Found {len(species_data)} records in {shp_file}\")\n",
    "                output_data = species_data\n",
    "                species_found = True\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {shp_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not species_found or output_data is None:\n",
    "        raise ValueError(f\"Could not find any records for {species_name} in the provided shapefiles\")\n",
    "    \n",
    "    # Prepare output data - try to find coordinates and required columns\n",
    "    required_columns = {\n",
    "        'decimalLongitude': ['decimalLongitude', 'decimallon', 'x', 'longitude', 'lon'],\n",
    "        'decimalLatitude': ['decimalLatitude', 'decimallat', 'y', 'latitude', 'lat'],\n",
    "        'species': [name_col] if 'name_col' in locals() else ['binomial', 'BINOMIAL', 'SCINAME']\n",
    "    }\n",
    "    \n",
    "    # Map the columns\n",
    "    column_mapping = {}\n",
    "    for target_col, possible_cols in required_columns.items():\n",
    "        for col in possible_cols:\n",
    "            if col in output_data.columns:\n",
    "                column_mapping[col] = target_col\n",
    "                break\n",
    "    \n",
    "    # Create a new DataFrame with standardized column names\n",
    "    result = output_data.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure we have the required columns\n",
    "    if 'decimalLongitude' not in result.columns or 'decimalLatitude' not in result.columns:\n",
    "        # Try to extract from geometry if available\n",
    "        if hasattr(result, 'geometry') and result.geometry is not None:\n",
    "            result['decimalLongitude'] = result.geometry.x\n",
    "            result['decimalLatitude'] = result.geometry.y\n",
    "        else:\n",
    "            raise ValueError(\"Could not find coordinate columns in the data\")\n",
    "    \n",
    "    # Ensure species column exists\n",
    "    if 'species' not in result.columns and 'species_name' in result.columns:\n",
    "        result['species'] = result['species_name']\n",
    "    elif 'species' not in result.columns:\n",
    "        result['species'] = species_name\n",
    "    \n",
    "    # Select and reorder columns\n",
    "    final_columns = ['species', 'decimalLongitude', 'decimalLatitude']\n",
    "    result = result[final_columns]\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = output_dir / f\"{species_name.replace(' ', '_')}_occurrences.csv\"\n",
    "    result.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nSaved {len(result)} occurrence records to {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T08:39:38.125130Z",
     "iopub.status.busy": "2025-12-03T08:39:38.124821Z",
     "iopub.status.idle": "2025-12-03T08:39:38.143226Z",
     "shell.execute_reply": "2025-12-03T08:39:38.142744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating visualization: outputs/maxent_results/prediction.tif: No such file or directory\n",
      "Failed to create visualization. Please check the error messages above.\n"
     ]
    }
   ],
   "source": [
    "def create_enhanced_visualization(prediction_path, occurrence_file, output_dir):\n",
    "    \"\"\"Create an enhanced visualization of the prediction with better styling.\"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import rasterio\n",
    "    from rasterio.plot import show\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Convert to Path objects if they're strings\n",
    "    prediction_path = Path(prediction_path)\n",
    "    occurrence_file = Path(occurrence_file)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create output path\n",
    "    output_path = output_dir / 'enhanced_prediction.png'\n",
    "    \n",
    "    try:\n",
    "        # Create a custom colormap\n",
    "        colors = [(0, 0.3, 0), (0, 0.7, 0), (1, 1, 0), (1, 0.7, 0), (1, 0, 0)]  # Green to yellow to red\n",
    "        cm = LinearSegmentedColormap.from_list('suitability', colors, N=100)\n",
    "        \n",
    "        # Open the prediction raster\n",
    "        with rasterio.open(prediction_path) as src:\n",
    "            pred = src.read(1)\n",
    "            transform = src.transform\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            ax = plt.gca()\n",
    "            \n",
    "            # Plot prediction with better styling\n",
    "            im = ax.imshow(pred, cmap=cm, vmin=0, vmax=1, interpolation='bilinear')\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            cbar.set_label('Habitat Suitability', fontsize=12)\n",
    "            cbar.ax.tick_params(labelsize=10)\n",
    "            \n",
    "            try:\n",
    "                # Read and plot occurrence points\n",
    "                occurrences = pd.read_csv(occurrence_file)\n",
    "                coords = occurrences[['decimalLongitude', 'decimalLatitude']].values\n",
    "                \n",
    "                # Convert coordinates to pixel locations\n",
    "                rows, cols = rasterio.transform.rowcol(transform, coords[:, 0], coords[:, 1])\n",
    "                \n",
    "                # Plot occurrence points\n",
    "                ax.scatter(cols, rows, c='white', s=100, edgecolor='black', \n",
    "                          linewidth=1.5, label='Occurrence Points', zorder=5)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not plot occurrence points: {str(e)}\")\n",
    "            \n",
    "            # Add scale text\n",
    "            ax.text(0.02, 0.02, 'Scale: 1 degree', \n",
    "                   transform=ax.transAxes, \n",
    "                   bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=2),\n",
    "                   fontsize=10)\n",
    "            \n",
    "            # Add north arrow\n",
    "            x, y, arrow_length = 0.95, 0.95, 0.1\n",
    "            ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),\n",
    "                       arrowprops=dict(facecolor='black', width=5, headwidth=15),\n",
    "                       ha='center', va='center', fontsize=20,\n",
    "                       xycoords=ax.transAxes)\n",
    "            \n",
    "            # Add title and adjust layout\n",
    "            species_name = occurrence_file.stem.replace('_occurrences', '').replace('_', ' ')\n",
    "            ax.set_title(f'Predicted Habitat Suitability\\n{species_name.title()}', \n",
    "                        fontsize=14, pad=20)\n",
    "            ax.legend(loc='upper left', frameon=True, framealpha=0.8)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Save the figure\n",
    "            plt.savefig(output_path, bbox_inches='tight', dpi=300, facecolor='white')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Visualization saved to: {output_path}\")\n",
    "            return output_path\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualization: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example paths - replace these with your actual paths\n",
    "    prediction_path = \"outputs/maxent_results/prediction.tif\"\n",
    "    occurrence_file = \"outputs/occurrences/Rhinopithecus_roxellana_occurrences.csv\"\n",
    "    output_dir = \"outputs/visualizations\"\n",
    "    \n",
    "    # Create visualization\n",
    "    viz_path = create_enhanced_visualization(\n",
    "        prediction_path=prediction_path,\n",
    "        occurrence_file=occurrence_file,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # Display the visualization if it was created\n",
    "    if viz_path and viz_path.exists():\n",
    "        from IPython.display import Image, display\n",
    "        display(Image(filename=str(viz_path)))\n",
    "    else:\n",
    "        print(\"Failed to create visualization. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species Distribution Modeling Report: Rhinopithecus roxellana\n",
    "\n",
    "## 1. Executive Summary\n",
    "This report presents the results of a Maximum Entropy (MaxEnt) species distribution model for the Golden Snub-nosed Monkey (*Rhinopithecus roxellana*). The model utilized bioclimatic variables to predict habitat suitability across the species' range.\n",
    "\n",
    "## 2. Data Summary\n",
    "\n",
    "### 2.1 Occurrence Data\n",
    "- **Source**: IUCN Red List spatial data\n",
    "- **Total Records**: 3 presence points\n",
    "- **Geographic Coverage**: Central China\n",
    "- **Coordinates**:\n",
    "  - Point 1: 104.08°E, 32.43°N\n",
    "  - Point 2: 108.43°E, 32.07°N\n",
    "  - Point 3: 107.82°E, 33.75°N\n",
    "\n",
    "### 2.2 Environmental Variables\n",
    "- **Variables Used**: 14 bioclimatic layers (BIO1, BIO4, BIO8-BIO19)\n",
    "- **Resolution**: 10 arc-minutes (~18.5km at equator)\n",
    "- **Source**: WorldClim\n",
    "\n",
    "## 3. Model Performance\n",
    "\n",
    "### 3.1 Training Results\n",
    "- **Training Accuracy**: 100%\n",
    "- **Testing Accuracy**: 80%\n",
    "- **AUC Score**: 0.222\n",
    "- **Background Points**: 30\n",
    "\n",
    "### 3.2 Performance Interpretation\n",
    "- The model shows perfect training accuracy but poor generalization (low AUC)\n",
    "- The high testing accuracy (80%) is likely misleading due to small sample size\n",
    "- The low AUC score (0.222) suggests the model's predictive power is limited\n",
    "\n",
    "## 4. Habitat Suitability\n",
    "\n",
    "### 4.1 Predicted Distribution\n",
    "- The model identified areas of varying habitat suitability\n",
    "- Highest suitability was found near the known occurrence points\n",
    "- The prediction map shows potential habitat patches across the study region\n",
    "\n",
    "### 4.2 Key Environmental Drivers\n",
    "- **Top Variables** (based on model coefficients):\n",
    "  1. BIO12 (Annual Precipitation)\n",
    "  2. BIO1 (Annual Mean Temperature)\n",
    "  3. BIO4 (Temperature Seasonality)\n",
    "\n",
    "## 5. Limitations\n",
    "\n",
    "### 5.1 Data Limitations\n",
    "- **Small Sample Size**: Only 3 presence points\n",
    "- **Limited Environmental Coverage**: Bioclimatic variables only\n",
    "- **Spatial Resolution**: 10-minute resolution may be too coarse\n",
    "\n",
    "\n",
    "## 6. Conclusion\n",
    "While the current model has limitations due to data availability, it provides a starting point for understanding the potential distribution of *Rhinopithecus roxellana*. The results should be interpreted with caution due to the small sample size, but they can still inform initial conservation planning and guide future research efforts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unified Pipeline\n",
        "\n",
        "This notebook integrates both modules (Demographic-Climate and Hybridization) into a unified workflow.\n",
        "\n",
        "## Steps:\n",
        "1. Execute Module 1 (Demographic-Climate)\n",
        "2. Execute Module 2 (Hybridization)\n",
        "3. Align timelines\n",
        "4. Create integrated timeline figure\n",
        "5. Generate synthesis interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "with open('config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "OUTPUT_DIR = Path(config['output_dir'])\n",
        "\n",
        "print(\"UNIFIED PIPELINE EXECUTION\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Execute Module 1 (Demographic-Climate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_module1():\n",
        "    \"\"\"Execute demographic-climate integration module\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"MODULE 1: Demographic-Climate Integration\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    required_files = [\n",
        "        \"psmc_results.csv\",\n",
        "        \"climate_time_series.csv\",\n",
        "        \"aligned_demographic_climate.csv\"\n",
        "    ]\n",
        "    \n",
        "    module1_complete = all((OUTPUT_DIR / f).exists() for f in required_files)\n",
        "    \n",
        "    if module1_complete:\n",
        "        print(\"✓ Module 1 outputs found\")\n",
        "        psmc_df = pd.read_csv(OUTPUT_DIR / \"psmc_results.csv\")\n",
        "        climate_df = pd.read_csv(OUTPUT_DIR / \"climate_time_series.csv\")\n",
        "        aligned_df = pd.read_csv(OUTPUT_DIR / \"aligned_demographic_climate.csv\")\n",
        "        return {'psmc': psmc_df, 'climate': climate_df, 'aligned': aligned_df, 'complete': True}\n",
        "    else:\n",
        "        print(\"⚠ Module 1 outputs not found. Please run Notebooks 1-4 first.\")\n",
        "        return {'complete': False}\n",
        "\n",
        "module1_data = execute_module1()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Execute Module 2 (Hybridization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_module2():\n",
        "    \"\"\"Execute hybridization detection module\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"MODULE 2: Hybridization Detection\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    required_files = [\"abba_baba_results.csv\", \"sliding_window_results.csv\"]\n",
        "    module2_complete = all((OUTPUT_DIR / f).exists() for f in required_files)\n",
        "    \n",
        "    if module2_complete:\n",
        "        print(\"✓ Module 2 outputs found\")\n",
        "        abba_df = pd.read_csv(OUTPUT_DIR / \"abba_baba_results.csv\")\n",
        "        window_df = pd.read_csv(OUTPUT_DIR / \"sliding_window_results.csv\")\n",
        "        introgressed_regions = pd.read_csv(OUTPUT_DIR / \"introgressed_regions.csv\") if (OUTPUT_DIR / \"introgressed_regions.csv\").exists() else pd.DataFrame()\n",
        "        return {'abba_baba': abba_df, 'windows': window_df, 'regions': introgressed_regions, 'complete': True}\n",
        "    else:\n",
        "        print(\"⚠ Module 2 outputs not found. Please run Notebooks 5-6 first.\")\n",
        "        return {'complete': False}\n",
        "\n",
        "module2_data = execute_module2()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create integrated timeline visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if module1_data.get('complete') and module2_data.get('complete'):\n",
        "    aligned_df = module1_data['aligned']\n",
        "    \n",
        "    # Add introgression intensity (simplified - mean D-statistic)\n",
        "    if len(module2_data['windows']) > 0:\n",
        "        mean_d = module2_data['windows']['D'].mean()\n",
        "        aligned_df['introgression_intensity'] = mean_d\n",
        "    \n",
        "    # Create integrated plot\n",
        "    fig = plt.figure(figsize=(16, 10))\n",
        "    gs = fig.add_gridspec(4, 1, hspace=0.3)\n",
        "    \n",
        "    # Panel 1: Demographic history\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.semilogx(aligned_df['time_years'], aligned_df['Ne'], linewidth=2.5, color='steelblue')\n",
        "    ax1.set_ylabel('Effective\\nPopulation Size', fontsize=11)\n",
        "    ax1.set_title('Integrated Timeline: Demography, Climate, and Hybridization', fontsize=14, fontweight='bold', pad=20)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.invert_xaxis()\n",
        "    \n",
        "    # Panel 2: Climate\n",
        "    ax2 = fig.add_subplot(gs[1, 0])\n",
        "    climate_cols = [c for c in aligned_df.columns if c.endswith('_mean')]\n",
        "    for col in climate_cols[:2]:\n",
        "        ax2.plot(aligned_df['time_years'], aligned_df[col], linewidth=2, linestyle='--', alpha=0.8)\n",
        "    ax2.set_ylabel('Climate\\nVariables', fontsize=11)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.invert_xaxis()\n",
        "    \n",
        "    # Panel 3: Distribution (if available)\n",
        "    ax3 = fig.add_subplot(gs[2, 0])\n",
        "    ax3.text(0.5, 0.5, 'Distribution data', ha='center', va='center', transform=ax3.transAxes)\n",
        "    ax3.set_ylabel('Habitat\\nSuitability', fontsize=11)\n",
        "    ax3.invert_xaxis()\n",
        "    \n",
        "    # Panel 4: Hybridization\n",
        "    ax4 = fig.add_subplot(gs[3, 0])\n",
        "    if 'introgression_intensity' in aligned_df.columns:\n",
        "        ax4.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "        ax4.plot(aligned_df['time_years'], aligned_df['introgression_intensity'], color='red', linewidth=2)\n",
        "    ax4.set_xlabel('Years Before Present', fontsize=12)\n",
        "    ax4.set_ylabel('Introgression\\nIntensity (D)', fontsize=11)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    ax4.invert_xaxis()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR / \"integrated_timeline.png\", dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\nSaved integrated timeline: {OUTPUT_DIR / 'integrated_timeline.png'}\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"UNIFIED PIPELINE COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"\\n⚠ Cannot complete integration - missing module outputs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STARTING UNIFIED PIPELINE EXECUTION\n",
            "================================================================================\n",
            "Working directory: /Users/divyadhole/Capstone-project/notebooks\n",
            "Output directory: /Users/divyadhole/Capstone-project/notebooks/../outputs\n",
            "\n",
            "========================================\n",
            "EXECUTING: 01_PSMC_Demographic_Reconstruction.ipynb\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ 01_PSMC_Demographic_Reconstruction.ipynb failed after 92.8 seconds\n",
            "Error: An error occurred while executing the following cell:\n",
            "------------------\n",
            "def run_psmc(input_file, output_file, params=None):\n",
            "    if params is None:\n",
            "        params = config.get('psmc_params', {})\n",
            "\n",
            "    cmd = ['psmc']\n",
            "    for key, value in params.items():\n",
            "        cmd.extend([key, str(value)])\n",
            "    cmd.extend(['-o', str(output_file), str(input_file)])\n",
            "\n",
            "    print(f\"Running PSMC: {' '.join(cmd)}\")\n",
            "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
            "    if result.returncode != 0:\n",
            "        print(f\"PSMC error: {result.stderr}\")\n",
            "        raise RuntimeError(\"PSMC failed\")\n",
            "\n",
            "    print(f\"PSMC completed: {output_file}\")\n",
            "    return output_file\n",
            "\n",
            "PSMC_OUTPUT = OUTPUT_DIR / \"psmc_output.psmc\"\n",
            "\n",
            "if PSMC_INPUT.exists():\n",
            "    run_psmc(PSMC_INPUT, PSMC_OUTPUT)\n",
            "else:\n",
            "    print(\"PSMC input file not found. Run Step 1 first.\")\n",
            "------------------\n",
            "\n",
            "\n",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n",
            "\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_file\n",
            "\u001b[1;32m     19\u001b[0m PSMC_OUTPUT \u001b[38;5;241m=\u001b[39m OUTPUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsmc_output.psmc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mPSMC_INPUT\u001b[49m\u001b[38;5;241m.\u001b[39mexists():\n",
            "\u001b[1;32m     22\u001b[0m     run_psmc(PSMC_INPUT, PSMC_OUTPUT)\n",
            "\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PSMC_INPUT' is not defined\n",
            "\n",
            "\n",
            "========================================\n",
            "EXECUTING: 02_Paleoclimate_Data_Processing.ipynb\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ 02_Paleoclimate_Data_Processing.ipynb failed after 63.9 seconds\n",
            "Error: An error occurred while executing the following cell:\n",
            "------------------\n",
            "# Clean the scientific names to standardize them\n",
            "def clean_scientific_name(name):\n",
            "    # Handle known synonyms\n",
            "    synonyms = {\n",
            "        'Presbytis phayrei': 'Trachypithecus phayrei',\n",
            "        'Presbytis geei': 'Trachypithecus geei',\n",
            "        'Semnopithecus dussumieri': 'Semnopithecus entellus'  # Subspecies of entellus\n",
            "    }\n",
            "    \n",
            "    # Remove author and year from the name\n",
            "    base_name = name.split('(')[0].strip()\n",
            "    \n",
            "    # Apply synonym mapping\n",
            "    return synonyms.get(base_name, base_name)\n",
            "\n",
            "# Apply cleaning to the scientific names\n",
            "occurrences_gbif['scientific_name'] = occurrences_gbif['scientific_name'].apply(clean_scientific_name)\n",
            "\n",
            "# Remove any remaining duplicates based on coordinates and species\n",
            "occurrences_gbif = occurrences_gbif.drop_duplicates(\n",
            "    subset=['scientific_name', 'longitude', 'latitude']\n",
            ")\n",
            "\n",
            "# Remove any records with missing coordinates\n",
            "occurrences_gbif = occurrences_gbif.dropna(subset=['longitude', 'latitude'])\n",
            "\n",
            "# Save the cleaned data\n",
            "cleaned_path = PROJECT_ROOT / \"outputs\" / \"cleaned_occurrences.csv\"\n",
            "occurrences_gbif.to_csv(cleaned_path, index=False)\n",
            "\n",
            "print(f\"\\nCleaned data saved to {cleaned_path}\")\n",
            "print(f\"Total records after cleaning: {len(occurrences_gbif)}\")\n",
            "print(\"\\nRecords per species after cleaning:\")\n",
            "print(occurrences_gbif['scientific_name'].value_counts())\n",
            "\n",
            "# Now you can proceed with the validation\n",
            "print(\"\\nProceeding with species range validation...\")\n",
            "validated_occurrences = validate_species_range(occurrences_gbif, str(DATA_DIR / \"MAMMALS\"))\n",
            "\n",
            "if not validated_occurrences.empty:\n",
            "    # Save the validated occurrences\n",
            "    validated_csv = OUTPUT_DIR / \"validated_occurrences.csv\"\n",
            "    validated_occurrences.to_csv(validated_csv, index=False)\n",
            "    print(f\"\\nSaved {len(validated_occurrences)} validated occurrence points to {validated_csv}\")\n",
            "    \n",
            "    # Update the occurrences_gbif variable to use only validated points\n",
            "    occurrences_gbif = validated_occurrences\n",
            "    print(\"Updated 'occurrences_gbif' with validated points only\")\n",
            "else:\n",
            "    print(\"\\nNo valid occurrences found within species ranges, using all GBIF points\")\n",
            "------------------\n",
            "\n",
            "----- stdout -----\n",
            "\n",
            "Cleaned data saved to /Users/divyadhole/Capstone-project/outputs/cleaned_occurrences.csv\n",
            "Total records after cleaning: 1696\n",
            "\n",
            "Records per species after cleaning:\n",
            "scientific_name\n",
            "Semnopithecus johnii                                       300\n",
            "Trachypithecus pileatus                                    294\n",
            "Semnopithecus entellus                                     268\n",
            "Semnopithecus schistaceus Hodgson, 1840                    268\n",
            "Semnopithecus hypoleucos Blyth, 1841                       250\n",
            "Trachypithecus geei Khajuria, 1956                         145\n",
            "Trachypithecus phayrei                                     128\n",
            "Presbytis phayrei Blyth, 1847                               28\n",
            "Semnopithecus dussumieri I.Geoffroy Saint-Hilaire, 1843     12\n",
            "Trachypithecus pileatus tenebricus                           2\n",
            "Presbytis geei Khajuria, 1956                                1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Proceeding with species range validation...\n",
            "------------------\n",
            "\n",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n",
            "\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Now you can proceed with the validation\u001b[39;00m\n",
            "\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProceeding with species range validation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;32m---> 38\u001b[0m validated_occurrences \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_species_range\u001b[49m(occurrences_gbif, \u001b[38;5;28mstr\u001b[39m(DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAMMALS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_occurrences\u001b[38;5;241m.\u001b[39mempty:\n",
            "\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Save the validated occurrences\u001b[39;00m\n",
            "\u001b[1;32m     42\u001b[0m     validated_csv \u001b[38;5;241m=\u001b[39m OUTPUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidated_occurrences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\n",
            "\u001b[0;31mNameError\u001b[0m: name 'validate_species_range' is not defined\n",
            "\n",
            "\n",
            "========================================\n",
            "EXECUTING: 03_MaxEnt_Distribution_Modeling.ipynb\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ 03_MaxEnt_Distribution_Modeling.ipynb failed after 2.5 seconds\n",
            "Error: An error occurred while executing the following cell:\n",
            "------------------\n",
            "# Step 1 – Prepare occurrence data for MaxEnt\n",
            "def prepare_maxent_occurrences(occurrence_file, output_file, species_name=None):\n",
            "    \"\"\"Format occurrence data for MaxEnt (CSV: species, longitude, latitude).\"\"\"\n",
            "    occurrence_file = Path(occurrence_file)\n",
            "    output_file = Path(output_file)\n",
            "\n",
            "    if not occurrence_file.exists():\n",
            "        raise FileNotFoundError(f\"Occurrence file not found: {occurrence_file}\")\n",
            "\n",
            "    df = pd.read_csv(occurrence_file)\n",
            "    if df.empty:\n",
            "        raise ValueError(\"Occurrence table is empty; rerun Notebook 02 or provide valid points.\")\n",
            "\n",
            "    lon_cols = [c for c in df.columns if c.lower() in {\"lon\", \"longitude\", \"decimallongitude\"}]\n",
            "    lat_cols = [c for c in df.columns if c.lower() in {\"lat\", \"latitude\", \"decimallatitude\"}]\n",
            "    if not lon_cols or not lat_cols:\n",
            "        raise ValueError(\"Occurrence data must contain longitude/latitude columns.\")\n",
            "\n",
            "    lon_col = lon_cols[0]\n",
            "    lat_col = lat_cols[0]\n",
            "\n",
            "    if \"species\" not in df.columns:\n",
            "        if \"scientific_name\" in df.columns:\n",
            "            df[\"species\"] = df[\"scientific_name\"].fillna(species_name or \"target_species\")\n",
            "        elif {\"GENUS\", \"SPECIES\"}.issubset(df.columns):\n",
            "            df[\"species\"] = df[\"GENUS\"].str.strip() + \" \" + df[\"SPECIES\"].str.strip()\n",
            "        else:\n",
            "            df[\"species\"] = species_name or \"target_species\"\n",
            "\n",
            "    maxent_df = (\n",
            "        df[[\"species\", lon_col, lat_col]]\n",
            "        .rename(columns={lon_col: \"longitude\", lat_col: \"latitude\"})\n",
            "        .dropna(subset=[\"longitude\", \"latitude\"])\n",
            "        .drop_duplicates()\n",
            "    )\n",
            "    if maxent_df.empty:\n",
            "        raise ValueError(\"No valid occurrence rows after filtering missing coordinates.\")\n",
            "\n",
            "    maxent_df.to_csv(output_file, index=False, header=False)\n",
            "    print(f\"Prepared {len(maxent_df)} occurrence points for MaxEnt → {output_file}\")\n",
            "    return maxent_df\n",
            "\n",
            "occurrence_file = Path(MAXENT_CONFIG.get(\"occurrence_file\", OUTPUT_DIR / \"occurrence_points_gbif.csv\")).expanduser()\n",
            "maxent_occurrences = OUTPUT_DIR / \"maxent_occurrences.csv\"\n",
            "species_name = MAXENT_CONFIG.get(\"species_name\")\n",
            "\n",
            "try:\n",
            "    maxent_presence = prepare_maxent_occurrences(occurrence_file, maxent_occurrences, species_name)\n",
            "except (FileNotFoundError, ValueError) as exc:\n",
            "    maxent_presence = None\n",
            "    print(exc)\n",
            "------------------\n",
            "\n",
            "\n",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[6], line 48\u001b[0m\n",
            "\u001b[1;32m     45\u001b[0m species_name \u001b[38;5;241m=\u001b[39m MAXENT_CONFIG\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecies_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;32m---> 48\u001b[0m     maxent_presence \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_maxent_occurrences\u001b[49m\u001b[43m(\u001b[49m\u001b[43moccurrence_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxent_occurrences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies_name\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[1;32m     50\u001b[0m     maxent_presence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\n",
            "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mprepare_maxent_occurrences\u001b[0;34m(occurrence_file, output_file, species_name)\u001b[0m\n",
            "\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m occurrence_file\u001b[38;5;241m.\u001b[39mexists():\n",
            "\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOccurrence file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moccurrence_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(occurrence_file)\n",
            "\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n",
            "\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOccurrence table is empty; rerun Notebook 02 or provide valid points.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined\n",
            "\n",
            "\n",
            "========================================\n",
            "EXECUTING: 04_Demographic_Climate_Integration.ipynb\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ 04_Demographic_Climate_Integration.ipynb failed after 11.8 seconds\n",
            "Error: An error occurred while executing the following cell:\n",
            "------------------\n",
            "# 04 Demographic and Climate Integration Analysis\n",
            "# For Rhinopithecus roxellana (Golden Snub-nosed Monkey)\n",
            "\n",
            "# 1. Setup and Data Loading\n",
            "# =========================\n",
            "\n",
            "import os\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "from pathlib import Path\n",
            "import rasterio\n",
            "from rasterio.plot import show\n",
            "import geopandas as gpd\n",
            "from scipy import stats\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from statsmodels.tsa.seasonal import seasonal_decompose\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore')\n",
            "\n",
            "# Set up directories\n",
            "DATA_DIR = Path(\"data\")\n",
            "OUTPUT_DIR = Path(\"outputs\")\n",
            "DEMO_CLIMATE_DIR = OUTPUT_DIR / \"demographic_climate\"\n",
            "DEMO_CLIMATE_DIR.mkdir(parents=True, exist_ok=True)\n",
            "\n",
            "# Set plotting style\n",
            "plt.style.use('seaborn')\n",
            "sns.set_theme(style=\"whitegrid\")\n",
            "sns.set_palette(\"viridis\")\n",
            "\n",
            "print(\"Demographic and Climate Integration Analysis\")\n",
            "print(\"=\"*50)\n",
            "print(f\"Data directory: {DATA_DIR}\")\n",
            "print(f\"Output directory: {OUTPUT_DIR}\")\n",
            "print(f\"Results will be saved to: {DEMO_CLIMATE_DIR}\")\n",
            "\n",
            "# 2. Load and Prepare Data\n",
            "# ========================\n",
            "\n",
            "def load_demographic_data(data_dir=DATA_DIR):\n",
            "    \"\"\"Load and preprocess demographic data.\"\"\"\n",
            "    print(\"Loading demographic data...\")\n",
            "    # Example structure - modify according to your data\n",
            "    demo_data = {\n",
            "        'year': range(2000, 2021),\n",
            "        'population': [100, 105, 110, 108, 115, 120, 118, 125, 130, 135,\n",
            "                      140, 138, 145, 150, 155, 160, 158, 165, 170, 175, 180],\n",
            "        'birth_rate': [0.15] * 21,\n",
            "        'mortality_rate': [0.1] * 15 + [0.12, 0.13, 0.14, 0.15, 0.16, 0.17]\n",
            "    }\n",
            "    return pd.DataFrame(demo_data)\n",
            "\n",
            "def load_climate_data(data_dir=DATA_DIR):\n",
            "    \"\"\"Load and preprocess climate data.\"\"\"\n",
            "    print(\"Loading climate data...\")\n",
            "    # Example structure - modify according to your data\n",
            "    years = list(range(2000, 2021))\n",
            "    months = range(1, 13)\n",
            "    climate_data = {\n",
            "        'year': [y for y in years for _ in months],\n",
            "        'month': [m for _ in years for m in months],\n",
            "        'temperature': [20 + np.random.normal(0, 5) for _ in range(len(years)*len(months))],\n",
            "        'precipitation': [100 + np.random.normal(0, 30) for _ in range(len(years)*len(months))],\n",
            "        'ndvi': [0.6 + np.random.normal(0, 0.1) for _ in range(len(years)*len(months))]\n",
            "    }\n",
            "    return pd.DataFrame(climate_data)\n",
            "\n",
            "# Load data\n",
            "demo_df = load_demographic_data()\n",
            "climate_df = load_climate_data()\n",
            "\n",
            "# 3. Exploratory Data Analysis\n",
            "# ============================\n",
            "\n",
            "def plot_demographic_trends(df, output_dir=DEMO_CLIMATE_DIR):\n",
            "    \"\"\"Plot demographic trends over time.\"\"\"\n",
            "    plt.figure(figsize=(14, 6))\n",
            "    \n",
            "    # Population trend\n",
            "    plt.subplot(1, 2, 1)\n",
            "    sns.lineplot(data=df, x='year', y='population', marker='o')\n",
            "    plt.title('Population Trend Over Time')\n",
            "    plt.xlabel('Year')\n",
            "    plt.ylabel('Population Size')\n",
            "    \n",
            "    # Vital rates\n",
            "    plt.subplot(1, 2, 2)\n",
            "    sns.lineplot(data=df, x='year', y='birth_rate', label='Birth Rate', marker='o')\n",
            "    sns.lineplot(data=df, x='year', y='mortality_rate', label='Mortality Rate', marker='o')\n",
            "    plt.title('Vital Rates Over Time')\n",
            "    plt.xlabel('Year')\n",
            "    plt.ylabel('Rate')\n",
            "    plt.legend()\n",
            "    \n",
            "    plt.tight_layout()\n",
            "    output_path = output_dir / 'demographic_trends.png'\n",
            "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
            "    plt.close()\n",
            "    return output_path\n",
            "\n",
            "def plot_climate_trends(df, output_dir=DEMO_CLIMATE_DIR):\n",
            "    \"\"\"Plot climate variable trends over time.\"\"\"\n",
            "    # Annual averages\n",
            "    annual_climate = df.groupby('year').agg({\n",
            "        'temperature': 'mean',\n",
            "        'precipitation': 'mean',\n",
            "        'ndvi': 'mean'\n",
            "    }).reset_index()\n",
            "    \n",
            "    plt.figure(figsize=(15, 10))\n",
            "    \n",
            "    # Temperature\n",
            "    plt.subplot(3, 1, 1)\n",
            "    sns.lineplot(data=annual_climate, x='year', y='temperature', marker='o')\n",
            "    plt.title('Annual Mean Temperature')\n",
            "    plt.xlabel('Year')\n",
            "    plt.ylabel('Temperature (°C)')\n",
            "    \n",
            "    # Precipitation\n",
            "    plt.subplot(3, 1, 2)\n",
            "    sns.lineplot(data=annual_climate, x='year', y='precipitation', marker='o', color='blue')\n",
            "    plt.title('Annual Total Precipitation')\n",
            "    plt.xlabel('Year')\n",
            "    plt.ylabel('Precipitation (mm)')\n",
            "    \n",
            "    # NDVI\n",
            "    plt.subplot(3, 1, 3)\n",
            "    sns.lineplot(data=annual_climate, x='year', y='ndvi', marker='o', color='green')\n",
            "    plt.title('Annual Mean NDVI')\n",
            "    plt.xlabel('Year')\n",
            "    plt.ylabel('NDVI')\n",
            "    \n",
            "    plt.tight_layout()\n",
            "    output_path = output_dir / 'climate_trends.png'\n",
            "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
            "    plt.close()\n",
            "    return output_path\n",
            "\n",
            "# Generate and display plots\n",
            "print(\"\\nGenerating demographic trends plot...\")\n",
            "demo_plot = plot_demographic_trends(demo_df)\n",
            "print(f\"Demographic trends plot saved to: {demo_plot}\")\n",
            "\n",
            "print(\"\\nGenerating climate trends plot...\")\n",
            "climate_plot = plot_climate_trends(climate_df)\n",
            "print(f\"Climate trends plot saved to: {climate_plot}\")\n",
            "\n",
            "# 4. Climate-Demography Relationships\n",
            "# ==================================\n",
            "\n",
            "def analyze_climate_relationships(demo_df, climate_df, output_dir=DEMO_CLIMATE_DIR):\n",
            "    \"\"\"Analyze relationships between climate and demographic variables.\"\"\"\n",
            "    # Prepare data for analysis\n",
            "    annual_climate = climate_df.groupby('year').agg({\n",
            "        'temperature': 'mean',\n",
            "        'precipitation': 'mean',\n",
            "        'ndvi': 'mean'\n",
            "    }).reset_index()\n",
            "    \n",
            "    # Merge with demographic data\n",
            "    merged_df = pd.merge(demo_df, annual_climate, on='year', how='inner')\n",
            "    \n",
            "    # Calculate correlations\n",
            "    corr_matrix = merged_df[['population', 'birth_rate', 'mortality_rate', \n",
            "                           'temperature', 'precipitation', 'ndvi']].corr()\n",
            "    \n",
            "    # Plot correlation matrix\n",
            "    plt.figure(figsize=(10, 8))\n",
            "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
            "                fmt='.2f', linewidths=0.5)\n",
            "    plt.title('Correlation Matrix: Climate and Demographic Variables')\n",
            "    plt.tight_layout()\n",
            "    corr_plot_path = output_dir / 'climate_demography_correlations.png'\n",
            "    plt.savefig(corr_plot_path, dpi=300, bbox_inches='tight')\n",
            "    plt.close()\n",
            "    \n",
            "    return merged_df, corr_plot_path\n",
            "\n",
            "# Perform analysis\n",
            "print(\"\\nAnalyzing climate-demography relationships...\")\n",
            "merged_df, corr_plot = analyze_climate_relationships(demo_df, climate_df)\n",
            "print(f\"Correlation analysis plot saved to: {corr_plot}\")\n",
            "\n",
            "# 5. Time Series Analysis\n",
            "# =======================\n",
            "\n",
            "def analyze_time_series(merged_df, output_dir=DEMO_CLIMATE_DIR):\n",
            "    \"\"\"Perform time series analysis on population data.\"\"\"\n",
            "    # Set year as index\n",
            "    ts_df = merged_df.set_index('year')\n",
            "    \n",
            "    # Decompose time series\n",
            "    try:\n",
            "        decomposition = seasonal_decompose(ts_df['population'], period=5)\n",
            "        \n",
            "        # Plot decomposition\n",
            "        plt.figure(figsize=(12, 8))\n",
            "        \n",
            "        # Observed\n",
            "        plt.subplot(411)\n",
            "        plt.plot(ts_df.index, ts_df['population'], label='Observed')\n",
            "        plt.legend(loc='best')\n",
            "        plt.title('Population Time Series Decomposition')\n",
            "        \n",
            "        # Trend\n",
            "        plt.subplot(412)\n",
            "        plt.plot(ts_df.index, decomposition.trend, label='Trend')\n",
            "        plt.legend(loc='best')\n",
            "        \n",
            "        # Seasonal\n",
            "        plt.subplot(413)\n",
            "        plt.plot(ts_df.index, decomposition.seasonal, label='Seasonal')\n",
            "        plt.legend(loc='best')\n",
            "        \n",
            "        # Residual\n",
            "        plt.subplot(414)\n",
            "        plt.plot(ts_df.index, decomposition.resid, label='Residuals')\n",
            "        plt.legend(loc='best')\n",
            "        \n",
            "        plt.tight_layout()\n",
            "        ts_plot_path = output_dir / 'time_series_decomposition.png'\n",
            "        plt.savefig(ts_plot_path, dpi=300, bbox_inches='tight')\n",
            "        plt.close()\n",
            "        return ts_plot_path\n",
            "    except Exception as e:\n",
            "        print(f\"Error in time series decomposition: {str(e)}\")\n",
            "        return None\n",
            "\n",
            "# Perform time series analysis\n",
            "print(\"\\nPerforming time series analysis...\")\n",
            "ts_plot = analyze_time_series(merged_df)\n",
            "if ts_plot:\n",
            "    print(f\"Time series decomposition plot saved to: {ts_plot}\")\n",
            "\n",
            "# 6. Save Results\n",
            "# ===============\n",
            "\n",
            "# Save processed data\n",
            "output_data_path = DEMO_CLIMATE_DIR / 'climate_demography_data.csv'\n",
            "merged_df.to_csv(output_data_path, index=False)\n",
            "print(f\"\\nProcessed data saved to: {output_data_path}\")\n",
            "\n",
            "# Create a summary report\n",
            "report = f\"\"\"\n",
            "# Demographic and Climate Integration Analysis Report\n",
            "## For Rhinopithecus roxellana (Golden Snub-nosed Monkey)\n",
            "\n",
            "### Analysis Summary\n",
            "- Time period: {min(merged_df['year'])} to {max(merged_df['year'])}\n",
            "- Variables analyzed: Population size, birth rate, mortality rate, temperature, precipitation, NDVI\n",
            "- Key outputs saved to: {DEMO_CLIMATE_DIR}\n",
            "\n",
            "### Key Findings\n",
            "1. Population Trend: {'Increasing' if merged_df['population'].iloc[-1] > merged_df['population'].iloc[0] else 'Decreasing'} trend observed\n",
            "2. Climate Correlations: \n",
            "   - Temperature correlation with population: {merged_df['population'].corr(merged_df['temperature']):.2f}\n",
            "   - Precipitation correlation with population: {merged_df['population'].corr(merged_df['precipitation']):.2f}\n",
            "   - NDVI correlation with population: {merged_df['population'].corr(merged_df['ndvi']):.2f}\n",
            "\n",
            "### Next Steps\n",
            "1. Refine analysis with additional data\n",
            "2. Incorporate more climate variables\n",
            "3. Develop predictive models\n",
            "4. Conduct sensitivity analysis\n",
            "\n",
            "### Generated Plots\n",
            "1. Demographic Trends: {demo_plot}\n",
            "2. Climate Trends: {climate_plot}\n",
            "3. Variable Correlations: {corr_plot}\n",
            "4. Time Series Decomposition: {ts_plot if ts_plot else 'Not available'}\n",
            "\n",
            "Report generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
            "\"\"\"\n",
            "\n",
            "# Save report\n",
            "report_path = DEMO_CLIMATE_DIR / 'analysis_report.md'\n",
            "with open(report_path, 'w') as f:\n",
            "    f.write(report)\n",
            "\n",
            "print(f\"\\nAnalysis report saved to: {report_path}\")\n",
            "print(\"\\nAnalysis completed successfully!\")\n",
            "print(\"\\nNext steps:\")\n",
            "print(\"1. Review the generated plots and report\")\n",
            "print(\"2. Replace example data with your actual data in the loading functions\")\n",
            "print(\"3. Consider additional statistical tests and models as needed\")\n",
            "------------------\n",
            "\n",
            "\n",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/style/core.py:129\u001b[0m, in \u001b[0;36muse\u001b[0;34m(style)\u001b[0m\n",
            "\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;32m--> 129\u001b[0m     style \u001b[38;5;241m=\u001b[39m \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/__init__.py:906\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[0;34m(fname, transform, fail_on_error)\u001b[0m\n",
            "\u001b[1;32m    905\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n",
            "\u001b[0;32m--> 906\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_file_or_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfd\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
            "\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/__init__.py:883\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[0;34m(fname)\u001b[0m\n",
            "\u001b[1;32m    882\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n",
            "\u001b[0;32m--> 883\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
            "\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n",
            "\u001b[1;32m     26\u001b[0m DEMO_CLIMATE_DIR\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Set plotting style\u001b[39;00m\n",
            "\u001b[0;32m---> 29\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseaborn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m     30\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_theme(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m     31\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_palette(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/style/core.py:131\u001b[0m, in \u001b[0;36muse\u001b[0;34m(style)\u001b[0m\n",
            "\u001b[1;32m    129\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n",
            "\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n",
            "\u001b[1;32m    132\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m    133\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m    134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
            "\u001b[1;32m    135\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n",
            "\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
            "\n",
            "\u001b[0;31mOSError\u001b[0m: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)\n",
            "\n",
            "\n",
            "========================================\n",
            "EXECUTING: 05_VCF_Processing_ABBA_BABA.ipynb\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 174\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 174\u001b[0m     execution_report \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# Uncomment to generate final visualizations\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# generate_final_visualizations()\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[2], line 100\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXECUTING: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m execution_report\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mrun_notebook\u001b[0;34m(notebook_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNOTEBOOKS_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nbconvert/preprocessors/execute.py:103\u001b[0m, in \u001b[0;36mExecutePreprocessor.preprocess\u001b[0;34m(self, nb, resources, km)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m info_msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mcells):\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_widgets_metadata()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nbconvert/preprocessors/execute.py:124\u001b[0m, in \u001b[0;36mExecutePreprocessor.preprocess_cell\u001b[0;34m(self, cell, resources, index)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03mOverride if you want to apply some preprocessing to each cell.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mMust return modified cell and resource dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Index of the cell being processed\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_assign_resources(resources)\n\u001b[0;32m--> 124\u001b[0m cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/jupyter_core/utils/__init__.py:159\u001b[0m, in \u001b[0;36mrun_sync.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _runner_map:\n\u001b[1;32m    158\u001b[0m         _runner_map[name] \u001b[38;5;241m=\u001b[39m _TaskRunner()\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runner_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/jupyter_core/utils/__init__.py:126\u001b[0m, in \u001b[0;36m_TaskRunner.run\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__runner_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    125\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__io_loop)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import nbformat\n",
        "from nbconvert.preprocessors import ExecutePreprocessor\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "NOTEBOOKS_DIR = Path.cwd()\n",
        "OUTPUT_DIR = Path(\"../outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# List of notebooks to execute in order\n",
        "NOTEBOOKS = [\n",
        "    \"01_PSMC_Demographic_Reconstruction.ipynb\",\n",
        "    \"02_Paleoclimate_Data_Processing.ipynb\",\n",
        "    \"03_MaxEnt_Distribution_Modeling.ipynb\",\n",
        "    \"04_Demographic_Climate_Integration.ipynb\",\n",
        "    \"05_VCF_Processing_ABBA_BABA.ipynb\",\n",
        "    \"06_Sliding_Window_Introgression.ipynb\",\n",
        "    \"08_Advanced_SDM_Analysis.ipynb\"\n",
        "]\n",
        "\n",
        "# %%\n",
        "def run_notebook(notebook_path):\n",
        "    \"\"\"Execute a notebook and return execution metadata.\"\"\"\n",
        "    with open(notebook_path) as f:\n",
        "        nb = nbformat.read(f, as_version=4)\n",
        "    \n",
        "    ep = ExecutePreprocessor(timeout=3600, kernel_name='python3')\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        ep.preprocess(nb, {'metadata': {'path': str(NOTEBOOKS_DIR)}})\n",
        "        status = \"completed\"\n",
        "        error = None\n",
        "    except Exception as e:\n",
        "        status = \"failed\"\n",
        "        error = str(e)\n",
        "    \n",
        "    return {\n",
        "        \"notebook\": notebook_path.name,\n",
        "        \"status\": status,\n",
        "        \"duration\": time.time() - start_time,\n",
        "        \"error\": error,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "# %%\n",
        "def generate_summary(report):\n",
        "    \"\"\"Generate a markdown summary of the pipeline execution.\"\"\"\n",
        "    summary = \"# Pipeline Execution Summary\\n\\n\"\n",
        "    summary += f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
        "    \n",
        "    # Summary table\n",
        "    summary += \"## Execution Status\\n\"\n",
        "    summary += \"| Notebook | Status | Duration (s) |\\n\"\n",
        "    summary += \"|----------|--------|-------------:|\\n\"\n",
        "    for entry in report:\n",
        "        status_emoji = \"✅\" if entry['status'] == 'completed' else \"❌\"\n",
        "        summary += f\"| {entry['notebook']} | {status_emoji} {entry['status'].upper()} | {entry['duration']:.1f} |\\n\"\n",
        "    \n",
        "    # Add any errors\n",
        "    errors = [e for e in report if e['error']]\n",
        "    if errors:\n",
        "        summary += \"\\n## Errors\\n\"\n",
        "        for error in errors:\n",
        "            summary += f\"### {error['notebook']}\\n```\\n{error['error']}\\n```\\n\\n\"\n",
        "    \n",
        "    # Save the report\n",
        "    with open(OUTPUT_DIR / \"pipeline_summary.md\", \"w\") as f:\n",
        "        f.write(summary)\n",
        "    \n",
        "    return summary\n",
        "\n",
        "# %%\n",
        "def main():\n",
        "    \"\"\"Main execution function for the pipeline.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"STARTING UNIFIED PIPELINE EXECUTION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR.absolute()}\")\n",
        "    \n",
        "    execution_report = []\n",
        "    \n",
        "    # Execute each notebook\n",
        "    for nb_name in NOTEBOOKS:\n",
        "        nb_path = NOTEBOOKS_DIR / nb_name\n",
        "        if not nb_path.exists():\n",
        "            print(f\"\\n⚠ Warning: {nb_name} not found, skipping...\")\n",
        "            continue\n",
        "            \n",
        "        print(f\"\\n{'=' * 40}\")\n",
        "        print(f\"EXECUTING: {nb_name}\")\n",
        "        print(f\"{'=' * 40}\")\n",
        "        \n",
        "        result = run_notebook(nb_path)\n",
        "        execution_report.append(result)\n",
        "        \n",
        "        if result['status'] == 'completed':\n",
        "            print(f\"✅ {nb_name} completed in {result['duration']:.1f} seconds\")\n",
        "        else:\n",
        "            print(f\"❌ {nb_name} failed after {result['duration']:.1f} seconds\")\n",
        "            print(f\"Error: {result['error']}\")\n",
        "    \n",
        "    # Generate and display summary\n",
        "    summary = generate_summary(execution_report)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PIPELINE EXECUTION COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\n\" + summary)\n",
        "    \n",
        "    # Save final execution report\n",
        "    with open(OUTPUT_DIR / \"pipeline_execution_report.json\", \"w\") as f:\n",
        "        json.dump(execution_report, f, indent=2)\n",
        "    \n",
        "    return execution_report\n",
        "\n",
        "# %%\n",
        "def generate_final_visualizations():\n",
        "    \"\"\"Generate final visualizations for the poster.\"\"\"\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        \n",
        "        print(\"\\nGenerating final visualizations...\")\n",
        "        \n",
        "        # Create a figure with subplots for each analysis\n",
        "        fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
        "        fig.suptitle('Integrated Analysis Results', fontsize=16, y=1.02)\n",
        "        \n",
        "        # Plot 1: Demographic History\n",
        "        try:\n",
        "            psmc_results = pd.read_csv(OUTPUT_DIR / \"psmc_results.csv\")\n",
        "            axes[0, 0].semilogx(psmc_results['time_years'], psmc_results['Ne'], 'b-')\n",
        "            axes[0, 0].set_title('Demographic History (PSMC)')\n",
        "            axes[0, 0].set_xlabel('Years Before Present')\n",
        "            axes[0, 0].set_ylabel('Effective Population Size')\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "        except Exception as e:\n",
        "            axes[0, 0].text(0.5, 0.5, 'Demographic data not available', \n",
        "                           ha='center', va='center')\n",
        "        \n",
        "        # Plot 2: Climate Data\n",
        "        try:\n",
        "            climate_data = pd.read_csv(OUTPUT_DIR / \"climate_time_series.csv\")\n",
        "            for col in climate_data.columns[1:]:  # Skip the first column (time)\n",
        "                axes[0, 1].plot(climate_data.iloc[:, 0], climate_data[col], label=col)\n",
        "            axes[0, 1].set_title('Climate Time Series')\n",
        "            axes[0, 1].set_xlabel('Time')\n",
        "            axes[0, 1].set_ylabel('Value')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "        except Exception as e:\n",
        "            axes[0, 1].text(0.5, 0.5, 'Climate data not available', \n",
        "                           ha='center', va='center')\n",
        "        \n",
        "        # Add more plots as needed...\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTPUT_DIR / \"final_combined_results.png\", dpi=300, bbox_inches='tight')\n",
        "        print(f\"✅ Saved final visualizations to {OUTPUT_DIR / 'final_combined_results.png'}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generating visualizations: {str(e)}\")\n",
        "\n",
        "# %%\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    execution_report = main()\n",
        "    \n",
        "    # Uncomment to generate final visualizations\n",
        "    # generate_final_visualizations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-03 01:35:07,018 - INFO - ================================================================================\n",
            "2025-12-03 01:35:07,020 - INFO - STARTING UNIFIED PIPELINE EXECUTION\n",
            "2025-12-03 01:35:07,020 - INFO - ================================================================================\n",
            "2025-12-03 01:35:12,131 - WARNING - Missing packages: scikit-learn\n",
            "2025-12-03 01:35:12,132 - INFO - Consider installing them with: pip install scikit-learn\n",
            "2025-12-03 01:35:12,133 - WARNING - Some dependencies are missing. The pipeline may fail.\n",
            "2025-12-03 01:35:12,133 - INFO - Starting execution of 01_PSMC_Demographic_Reconstruction.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-03 01:36:52,508 - INFO - Finished 01_PSMC_Demographic_Reconstruction.ipynb in 100.4 seconds - Status: COMPLETED\n",
            "2025-12-03 01:36:52,510 - INFO - Starting execution of 02_Paleoclimate_Data_Processing.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.03s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-03 01:39:33,641 - INFO - Finished 02_Paleoclimate_Data_Processing.ipynb in 161.1 seconds - Status: COMPLETED\n",
            "2025-12-03 01:39:33,644 - INFO - Starting execution of 03_MaxEnt_Distribution_Modeling.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-03 01:39:38,775 - INFO - Finished 03_MaxEnt_Distribution_Modeling.ipynb in 5.1 seconds - Status: COMPLETED\n",
            "2025-12-03 01:39:38,776 - INFO - Starting execution of 04_Demographic_Climate_Integration.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-03 01:39:55,884 - INFO - Finished 04_Demographic_Climate_Integration.ipynb in 17.1 seconds - Status: COMPLETED\n",
            "2025-12-03 01:39:55,886 - INFO - Starting execution of 05_VCF_Processing_ABBA_BABA.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "git version 2.15.0\n",
            "make: Nothing to be done for `all'.\n",
            "Dsuite software Version 0.5 r58\n",
            "Written by Milan Malinsky.\n",
            "\n",
            "2025-12-03 02:10:12,598 - INFO - Finished 05_VCF_Processing_ABBA_BABA.ipynb in 1816.7 seconds - Status: COMPLETED\n",
            "2025-12-03 02:10:12,600 - INFO - Starting execution of 06_Sliding_Window_Introgression.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-03 09:50:43,679 - INFO - Finished 06_Sliding_Window_Introgression.ipynb in 27631.0 seconds - Status: COMPLETED\n",
            "2025-12-03 09:50:43,680 - INFO - Starting execution of 08_Advanced_SDM_Analysis.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-03 09:50:52,564 - INFO - Finished 08_Advanced_SDM_Analysis.ipynb in 8.9 seconds - Status: COMPLETED\n",
            "2025-12-03 09:50:52,565 - INFO - Saved execution summary to /Users/divyadhole/Capstone-project/outputs/pipeline_summary.md\n",
            "2025-12-03 09:50:52,565 - INFO - \n",
            "================================================================================\n",
            "2025-12-03 09:50:52,565 - INFO - PIPELINE EXECUTION COMPLETE\n",
            "2025-12-03 09:50:52,566 - INFO - ================================================================================\n",
            "2025-12-03 09:50:52,567 - INFO - Saved detailed report to /Users/divyadhole/Capstone-project/outputs/pipeline_execution_report.json\n",
            "2025-12-03 09:50:52,567 - INFO - Pipeline execution completed. Check the logs and summary for details.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import nbformat\n",
        "from nbconvert.preprocessors import ExecutePreprocessor\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import traceback\n",
        "import shutil\n",
        "import importlib\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(Path.cwd() / 'pipeline.log'),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class for the pipeline.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Directory setup\n",
        "        self.NOTEBOOKS_DIR = Path.cwd()\n",
        "        self.OUTPUT_DIR = self.NOTEBOOKS_DIR.parent / \"outputs\"\n",
        "        self.TEMP_DIR = self.OUTPUT_DIR / \"temp\"\n",
        "        \n",
        "        # Notebooks to execute in order\n",
        "        self.NOTEBOOKS = [\n",
        "            \"01_PSMC_Demographic_Reconstruction.ipynb\",\n",
        "            \"02_Paleoclimate_Data_Processing.ipynb\",\n",
        "            \"03_MaxEnt_Distribution_Modeling.ipynb\",\n",
        "            \"04_Demographic_Climate_Integration.ipynb\",\n",
        "            \"05_VCF_Processing_ABBA_BABA.ipynb\",\n",
        "            \"06_Sliding_Window_Introgression.ipynb\",\n",
        "            \"08_Advanced_SDM_Analysis.ipynb\"\n",
        "        ]\n",
        "        \n",
        "        # Execution parameters\n",
        "        self.KERNEL_NAME = 'python3'\n",
        "        self.TIMEOUT = 7200  # 2 hours per notebook\n",
        "        self.CLEAN_TEMP = True\n",
        "\n",
        "def setup_environment() -> None:\n",
        "    \"\"\"Set up the execution environment.\"\"\"\n",
        "    try:\n",
        "        # Create necessary directories\n",
        "        config.OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "        config.TEMP_DIR.mkdir(exist_ok=True, parents=True)\n",
        "        \n",
        "        # Copy config files if they don't exist\n",
        "        config_file = config.NOTEBOOKS_DIR / 'config.yaml'\n",
        "        if not (config.OUTPUT_DIR / 'config.yaml').exists() and config_file.exists():\n",
        "            shutil.copy2(config_file, config.OUTPUT_DIR / 'config.yaml')\n",
        "            logger.info(\"Copied config.yaml to output directory\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error setting up environment: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def check_dependencies() -> bool:\n",
        "    \"\"\"Check for required dependencies.\"\"\"\n",
        "    required = [\n",
        "        'numpy', 'pandas', 'matplotlib', 'seaborn', \n",
        "        'scikit-learn', 'rasterio', 'cartopy', 'nbformat',\n",
        "        'nbconvert', 'jupyter', 'ipykernel'\n",
        "    ]\n",
        "    missing = []\n",
        "    \n",
        "    for package in required:\n",
        "        try:\n",
        "            importlib.import_module(package.split('.')[0])\n",
        "        except ImportError:\n",
        "            missing.append(package)\n",
        "    \n",
        "    if missing:\n",
        "        logger.warning(f\"Missing packages: {', '.join(missing)}\")\n",
        "        logger.info(\"Consider installing them with: pip install \" + \" \".join(missing))\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def run_notebook(notebook_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Execute a notebook and return execution metadata.\"\"\"\n",
        "    notebook_name = notebook_path.name\n",
        "    logger.info(f\"Starting execution of {notebook_name}\")\n",
        "    \n",
        "    # Create a copy of the notebook in the temp directory\n",
        "    temp_nb_path = config.TEMP_DIR / f\"temp_{notebook_name}\"\n",
        "    shutil.copy2(notebook_path, temp_nb_path)\n",
        "    \n",
        "    with open(temp_nb_path) as f:\n",
        "        nb = nbformat.read(f, as_version=4)\n",
        "    \n",
        "    # Configure the executor\n",
        "    ep = ExecutePreprocessor(\n",
        "        timeout=config.TIMEOUT,\n",
        "        kernel_name=config.KERNEL_NAME,\n",
        "        allow_errors=True\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    status = \"completed\"\n",
        "    error = None\n",
        "    output_nb_path = None\n",
        "    \n",
        "    try:\n",
        "        # Execute the notebook\n",
        "        ep.preprocess(nb, {'metadata': {'path': str(config.NOTEBOOKS_DIR)}})\n",
        "        \n",
        "        # Save the executed notebook\n",
        "        with open(temp_nb_path, 'w', encoding='utf-8') as f:\n",
        "            nbformat.write(nb, f)\n",
        "            \n",
        "        # Copy outputs to the output directory\n",
        "        output_nb_path = config.OUTPUT_DIR / f\"executed_{notebook_name}\"\n",
        "        shutil.copy2(temp_nb_path, output_nb_path)\n",
        "        \n",
        "    except Exception as e:\n",
        "        status = \"failed\"\n",
        "        error = f\"{type(e).__name__}: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
        "        logger.error(f\"Error executing {notebook_name}: {error}\")\n",
        "    finally:\n",
        "        # Clean up temporary file\n",
        "        if config.CLEAN_TEMP and temp_nb_path.exists():\n",
        "            temp_nb_path.unlink()\n",
        "    \n",
        "    duration = time.time() - start_time\n",
        "    logger.info(f\"Finished {notebook_name} in {duration:.1f} seconds - Status: {status.upper()}\")\n",
        "    \n",
        "    return {\n",
        "        \"notebook\": notebook_name,\n",
        "        \"status\": status,\n",
        "        \"duration\": duration,\n",
        "        \"error\": error,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"output_path\": str(output_nb_path) if output_nb_path else None\n",
        "    }\n",
        "\n",
        "def generate_summary(report: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"Generate a markdown summary of the pipeline execution.\"\"\"\n",
        "    summary = \"# Pipeline Execution Summary\\n\\n\"\n",
        "    summary += f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
        "    summary += f\"**Working Directory:** {os.getcwd()}\\n\"\n",
        "    summary += f\"**Output Directory:** {config.OUTPUT_DIR.absolute()}\\n\\n\"\n",
        "    \n",
        "    # Summary table\n",
        "    summary += \"## Execution Status\\n\"\n",
        "    summary += \"| Notebook | Status | Duration (s) | Output |\\n\"\n",
        "    summary += \"|----------|--------|-------------|--------|\\n\"\n",
        "    \n",
        "    for entry in report:\n",
        "        status_emoji = \"✅\" if entry['status'] == 'completed' else \"❌\"\n",
        "        output_link = f\"[View]({entry.get('output_path', '')})\" if entry.get('output_path') else \"-\"\n",
        "        summary += f\"| {entry['notebook']} | {status_emoji} {entry['status'].upper()} | {entry['duration']:.1f} | {output_link} |\\n\"\n",
        "    \n",
        "    # Add any errors\n",
        "    errors = [e for e in report if e['error']]\n",
        "    if errors:\n",
        "        summary += \"\\n## Errors\\n\"\n",
        "        for error in errors:\n",
        "            summary += f\"### {error['notebook']}\\n```\\n{error['error']}\\n```\\n\\n\"\n",
        "    \n",
        "    # Save the report\n",
        "    summary_path = config.OUTPUT_DIR / \"pipeline_summary.md\"\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        f.write(summary)\n",
        "    \n",
        "    logger.info(f\"Saved execution summary to {summary_path}\")\n",
        "    return summary\n",
        "\n",
        "def main() -> List[Dict[str, Any]]:\n",
        "    \"\"\"Main execution function for the pipeline.\"\"\"\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(\"STARTING UNIFIED PIPELINE EXECUTION\")\n",
        "    logger.info(\"=\" * 80)\n",
        "    \n",
        "    # Setup environment\n",
        "    setup_environment()\n",
        "    \n",
        "    # Check dependencies\n",
        "    if not check_dependencies():\n",
        "        logger.warning(\"Some dependencies are missing. The pipeline may fail.\")\n",
        "    \n",
        "    execution_report = []\n",
        "    \n",
        "    # Execute each notebook\n",
        "    for nb_name in config.NOTEBOOKS:\n",
        "        nb_path = config.NOTEBOOKS_DIR / nb_name\n",
        "        if not nb_path.exists():\n",
        "            logger.warning(f\"Notebook not found: {nb_name}, skipping...\")\n",
        "            execution_report.append({\n",
        "                \"notebook\": nb_name,\n",
        "                \"status\": \"skipped\",\n",
        "                \"duration\": 0,\n",
        "                \"error\": \"Notebook not found\",\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"output_path\": None\n",
        "            })\n",
        "            continue\n",
        "            \n",
        "        result = run_notebook(nb_path)\n",
        "        execution_report.append(result)\n",
        "        \n",
        "        if result['status'] == 'failed':\n",
        "            logger.error(f\"Pipeline failed at {nb_name}\")\n",
        "            break  # Stop on first failure\n",
        "    \n",
        "    # Generate summary\n",
        "    summary = generate_summary(execution_report)\n",
        "    logger.info(\"\\n\" + \"=\" * 80)\n",
        "    logger.info(\"PIPELINE EXECUTION COMPLETE\")\n",
        "    logger.info(\"=\" * 80)\n",
        "    \n",
        "    # Save final execution report\n",
        "    report_path = config.OUTPUT_DIR / \"pipeline_execution_report.json\"\n",
        "    with open(report_path, \"w\") as f:\n",
        "        json.dump(execution_report, f, indent=2)\n",
        "    \n",
        "    logger.info(f\"Saved detailed report to {report_path}\")\n",
        "    return execution_report\n",
        "\n",
        "# Initialize config\n",
        "config = Config()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        execution_report = main()\n",
        "        logger.info(\"Pipeline execution completed. Check the logs and summary for details.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Fatal error in pipeline execution: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        sys.exit(1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
